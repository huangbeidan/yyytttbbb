<html>
<title> - ALGORITHMS: HOW COMPANIES' DECISIONS ABOUT DATA AND CONTENT IMPACT CONSUMERS</title>
<body><pre>
[House Hearing, 115 Congress]
[From the U.S. Government Publishing Office]






  ALGORITHMS: HOW COMPANIES' DECISIONS ABOUT DATA AND CONTENT IMPACT 
                               CONSUMERS

=======================================================================

                             JOINT HEARING

                               BEFORE THE

             SUBCOMMITTEE ON COMMUNICATIONS AND TECHNOLOGY

                                AND THE

        SUBCOMMITTEE ON DIGITAL COMMERCE AND CONSUMER PROTECTION

                                 OF THE

                    COMMITTEE ON ENERGY AND COMMERCE
                        HOUSE OF REPRESENTATIVES

                     ONE HUNDRED FIFTEENTH CONGRESS

                             FIRST SESSION

                               __________

                           NOVEMBER 29, 2017

                               __________

                           Serial No. 115-80


[GRAPHIC(S) NOT AVAILABLE IN TIFF FORMAT]





      Printed for the use of the Committee on Energy and Commerce
                       energycommerce.house.gov
                                   ______
		 
                     U.S. GOVERNMENT PUBLISHING OFFICE 
		 
28-578 PDF                WASHINGTON : 2018                 

































                    COMMITTEE ON ENERGY AND COMMERCE

                          GREG WALDEN, Oregon
                                 Chairman

JOE BARTON, Texas                    FRANK PALLONE, Jr., New Jersey
  Vice Chairman                        Ranking Member
FRED UPTON, Michigan                 BOBBY L. RUSH, Illinois
JOHN SHIMKUS, Illinois               ANNA G. ESHOO, California
MICHAEL C. BURGESS, Texas            ELIOT L. ENGEL, New York
MARSHA BLACKBURN, Tennessee          GENE GREEN, Texas
STEVE SCALISE, Louisiana             DIANA DeGETTE, Colorado
ROBERT E. LATTA, Ohio                MICHAEL F. DOYLE, Pennsylvania
CATHY McMORRIS RODGERS, Washington   JANICE D. SCHAKOWSKY, Illinois
GREGG HARPER, Mississippi            G.K. BUTTERFIELD, North Carolina
LEONARD LANCE, New Jersey            DORIS O. MATSUI, California
BRETT GUTHRIE, Kentucky              KATHY CASTOR, Florida
PETE OLSON, Texas                    JOHN P. SARBANES, Maryland
DAVID B. McKINLEY, West Virginia     JERRY McNERNEY, California
ADAM KINZINGER, Illinois             PETER WELCH, Vermont
H. MORGAN GRIFFITH, Virginia         BEN RAY LUJAN, New Mexico
GUS M. BILIRAKIS, Florida            PAUL TONKO, New York
BILL JOHNSON, Ohio                   YVETTE D. CLARKE, New York
BILLY LONG, Missouri                 DAVID LOEBSACK, Iowa
LARRY BUCSHON, Indiana               KURT SCHRADER, Oregon
BILL FLORES, Texas                   JOSEPH P. KENNEDY, III, 
SUSAN W. BROOKS, Indiana             Massachusetts
MARKWAYNE MULLIN, Oklahoma           TONY CARDENAS, California
RICHARD HUDSON, North Carolina       RAUL RUIZ, California
CHRIS COLLINS, New York              SCOTT H. PETERS, California
KEVIN CRAMER, North Dakota           DEBBIE DINGELL, Michigan
TIM WALBERG, Michigan
MIMI WALTERS, California
RYAN A. COSTELLO, Pennsylvania
EARL L. ``BUDDY'' CARTER, Georgia
JEFF DUNCAN, South Carolina

                                  (ii)
             Subcommittee on Communications and Technology

                      MARSHA BLACKBURN, Tennessee
                                 Chairman
LEONARD LANCE, New Jersey            MICHAEL F. DOYLE, Pennsylvania
  Vice Chairman                        Ranking Member
JOHN SHIMKUS, Illinois               PETER WELCH, Vermont
STEVE SCALISE, Louisiana             YVETTE D. CLARKE, New York
ROBERT E. LATTA, Ohio                DAVID LOEBSACK, Iowa
BRETT GUTHRIE, Kentucky              RAUL RUIZ, California
PETE OLSON, Texas                    DEBBIE DINGELL, Michigan
ADAM KINZINGER, Illinois             BOBBY L. RUSH, Illinois
GUS M. BILIRAKIS, Florida            ANNA G. ESHOO, California
BILL JOHNSON, Ohio                   ELIOT L. ENGEL, New York
BILLY LONG, Missouri                 G.K. BUTTERFIELD, North Carolina
BILL FLORES, Texas                   DORIS O. MATSUI, California
SUSAN W. BROOKS, Tennessee           JERRY McNERNEY, California
CHRIS COLLINS, New York              FRANK PALLONE, Jr., New Jersey (ex 
KEVIN CRAMER, North Dakota               officio)
MIMI WALTERS, California
RYAN A. COSTELLO, Pennsylvania
GREG WALDEN, Oregon (ex officio)
                                 ------                                

        Subcommittee on Digital Commerce and Consumer Protection

                         ROBERT E. LATTA, Ohio
                                 Chairman
ADAM KINZINGER, Illinois             JANICE D. SCHAKOWSKY, Illinois
  Vice Chairman                        Ranking Member
FRED UPTON, Michigan                 BEN RAY LUJAN, New Mexico
MICHAEL C. BURGESS, Texas            YVETTE D. CLARKE, New York
LEONARD LANCE, New Jersey            TONY CARDENAS, California
BRETT GUTHRIE, Kentucky              DEBBIE DINGELL, Michigan
DAVID B. McKINLEY, West Virgina      DORIS O. MATSUI, California
ADAM KINZINGER, Illinois             PETER WELCH, Vermont
GUS M. BILIRAKIS, Florida            JOSEPH P. KENNEDY, III, 
LARRY BUCSHON, Indiana                   Massachusetts
MARKWAYNE MULLIN, Oklahoma           GENE GREEN, Texas
MIMI WALTERS, California             FRANK PALLONE, Jr., New Jersey (ex 
RYAN A. COSTELLO, Pennsylvania           officio)
GREG WALDEN, Oregon (ex officio)



























                             C O N T E N T S

                              ----------                              
                                                                   Page
Hon. Robert E. Latta, a Representative in Congress from the State 
  of Ohio, opening statement.....................................     2
    Prepared statement...........................................     3
Hon. Janice D. Schakowsky, a Representative in Congress from the 
  State of Illinois, opening statement...........................     4
Hon. Marsha Blackburn, a Representative in Congress from the 
  State of Tennessee, opening statement..........................     5
    Prepared statement...........................................     7
Hon. Michael F. Doyle, a Representative in Congress from the 
  Commonwealth of Pennsylvania, opening statement................     7
Hon. Greg Walden, a Representative in Congress from the State of 
  Oregon, opening statement......................................     9
    Prepared statement...........................................    11
Hon. Frank Pallone, Jr., a Representative in Congress from the 
  State of New Jersey, opening statement.........................    12
    Prepared statement...........................................    13

                               Witnesses

Omri Ben-Shahar, Ph.D., Leo Herzel Professor in Law, University 
  of Chicago Law School..........................................    15
    Prepared statement...........................................    18
    Answers to submitted questions...............................   154
Kate Klonick, Resident Fellow, Information Society Project, Yale 
  Law School.....................................................    23
    Prepared statement...........................................    25
    Answers to submitted questions \1\...........................   157
Laura Moy, Deputy Director, Center on Privacy & Technology at 
  Georgetown Law.................................................    33
    Prepared statement...........................................    35
    Answers to submitted questions...............................   159
Catherine Tucker, Ph.D., Sloane Distinguished Professor of 
  Management Science, MIT Sloane School of Management............    57
    Prepared statement...........................................    59
    Answers to submitted questions...............................   164
Frank Pasquale, Professor of Law, University of Maryland.........    67
    Prepared statement...........................................    69
    Answers to submitted questions...............................   169
Michael Kearns, Ph.D., Computer and Information Science 
  Professor, University of Pennsylvania..........................    93
    Prepared statement...........................................    95
    Answers to submitted questions...............................   183

                           Submitted Material

Letter of November 1, 2016, from Hon. Robin L. Kelly, et al., to 
  Mark Zuckerberg, Chairman and Chief Executive Officer, 
  Facebook, submitted by Ms. Clarke..............................   130
Statement of Frank Pasquale, Professor of Law, University of 
  Maryland, before the United States Senate, September 12, 2017, 
  submitted by Ms. Matsui........................................   131

----------
\1\ Ms. Klonick did not answer submitted questions for the record 
  by the time of printing.
Tweets of November 22, 2017, Cloudflare, submitted by Mr. 
  Costello.......................................................   149
Report of May 2016, ``Online Privacy and ISPs,'' The Institute 
  for Information Security & Privacy,\1\  submitted by Mr. Lance
Letter of November 28, 2017, from Marc Rotenberg, President, and 
  Caitriona Fitzgerald, Policy Director, Electronic Privacy 
  Information Center, to Mr. Latta, et al., submitted by Mr. 
  Lance..........................................................   150

----------
\1\ The information has been retained in committee files and also 
  is available at  http://docs.house.gov/meetings/IF/IF17/
  20171129/106659/HHRG-115-IF17-20171129-SD004-U4.pdf.

 
  ALGORITHMS: HOW COMPANIES' DECISIONS ABOUT DATA AND CONTENT IMPACT 
                               CONSUMERS

                              ----------                              


                      WEDNESDAY, NOVEMBER 29, 2017

                  House of Representatives,
      Subcommittee on Communications and Technology
                             joint with the
     Subcommittee on Digital Commerce and Consumer 
                                        Protection,
                          Committee on Energy and Commerce,
                                                    Washington, DC.
    The subcommittee met, pursuant to call, at 10:07 a.m., in 
room 2123, Rayburn House Office Building, Hon. Robert E. Latta 
(chairman of the Subcommittee on Digital Commerce and Consumer 
Protection) presiding.
    Members present: Representatives Latta, Blackburn, Harper, 
Lance, Shimkus, Burgess, Guthrie, Olson, Kinzinger, Bilirakis, 
Johnson, Bucshon, Flores, Brooks, Mullin, Collins, Cramer, 
Walters, Costello, Walden (ex officio), Doyle, Schakowsky, 
Eshoo, Engel, Green, Matsui, McNerney, Welch, Clarke, Loebsack, 
Ruiz, Dingell, and Pallone (ex officio).
    Staff present: Mike Bloomquist, Deputy Staff Director; 
Samantha Bopp, Staff Assistant; Kelly Collins, Staff Assistant; 
Robin Colwell, Chief Counsel, Communications and Technology; 
Sean Farrell, Professional Staff Member, Communications and 
Technology; Margaret T. Fogarty, Staff Assistant; Melissa 
Froelich, Chief Counsel, Digital Commerce and Consumer 
Protection; Adam Fromm, Director of Outreach and Coalitions; 
Gene Fullano, Detailee, Communications and Technology; Ali 
Fulling, Legislative Clerk, Oversight and Investigations, 
Digital Commerce and Consumer Protection; Theresa Gambo, Human 
Resources and Office Administrator; Elena Hernandez, Press 
Secretary; Paul Jackson, Professional Staff Member, Digital 
Commerce and Consumer Protection; Bijan Koohmaraie, Counsel, 
Digital Commerce and Consumer Protection; Tim Kurth, Senior 
Professional Staff, Communications and Technology; Lauren 
McCarty, Counsel, Communications and Technology; Katie McKeogh, 
Press Assistant; Alex Miller, Video Production Aide and Press 
Assistant; Mark Ratner, Policy Coordinator; Madeline Vey, 
Policy Coordinator, Digital Commerce and Consumer Protection; 
Evan Viau, Legislative Clerk, Communications and Technology; 
Hamlin Wade, Special Advisor for External Affairs; Everett 
Winnick, Director of Information Technology; Greg Zerzan, 
Counsel, Digital Commerce and Consumer Protection; Michelle 
Ash, Minority Chief Counsel, Digital Commerce and Consumer 
Protection; Jeff Carroll, Minority Staff Director; David 
Goldman, Minority Chief Counsel, Communications and Technology; 
Lisa Goldman, Minority Counsel; Lori Maarbjerg, Minority FCC 
Detailee; Dan Miller, Minority Policy Analyst; Caroline Paris-
Behr, Minority Policy Analyst; and C.J. Young, Minority Press 
Secretary.

OPENING STATEMENT OF HON. ROBERT E. LATTA, A REPRESENTATIVE IN 
                CONGRESS FROM THE STATE OF OHIO

    Mr. Latta. Well, good morning. I would like to call our 
joint subcommittee meeting to order, and the Chair recognizes 
himself for 5 minutes for an opening statement.
    And good morning again. I would like to welcome everyone 
back from Thanksgiving holiday to our joint subcommittee 
hearing. I would like to thank our witnesses for being here 
today. I would venture to guess many people were able to get a 
jumpstart on their holiday shopping and seeing some of the 
earlier reports showing that online shopping rose 17 percent 
from last year, which makes our hearing this morning even more 
timely.
    When Chairman Walden became chairman of the Energy and 
Commerce Committee, we agreed that keeping our focus on the 
consumer was a priority for the committee. And everything that 
the Digital Commerce and Consumer Protection Subcommittee has 
done, whether it has been exploring new technologies through 
our Disrupter Series or the bipartisan work that went into the 
SELF DRIVE Act, our goal has always been to act in the best 
interest of the consumer, the American people.
    Earlier this fall, the Equifax data breach compromised the 
personal information of over 145 million Americans. This 
troubling incident raised many questions about credit industry 
practices with respect to the collection of consumer 
information. Many Americans, some of whom never heard of 
Equifax, were confused as to how their sensitive personal 
information could have been compromised by a company they had 
never interacted with.
    Just last week, Uber announced their systems were hacked, 
exposing data of over 57 million users. Rather than alert 
authorities and make the breach known to their users and 
drivers, Uber kept the hack secret for a year. Disregard of law 
and disregard of consumers' and drivers' trust all require 
close scrutiny. The Digital Commerce and Consumer Protection 
Subcommittee will continue our work to protect consumers and 
make sure those who disregard the law are held accountable.
    As investigations continue, the importance of this hearing 
cannot be understated. Polls show Americans both feel that 
technology has had a positive effect on our society but are 
also skeptical about how their information is used by major 
technology companies. As policymakers, it is our obligation to 
ask the tough questions and make sure consumers understand how 
their information is being used in our digitally driven 
economy.
    That is why we explore today how personal information about 
consumers is collected online and, importantly, how companies 
use that information to make decisions about the content 
consumers see. Right now, there are more than 224 million smart 
phone users in America, and U.S. consumers spend about 5 hours 
a day on their mobile devices. As we continue to see the number 
of connected devices increase and our digital economy expand, 
Americans are only going to spend more and more time online 
browsing the web, shopping, or checking social media, with more 
information about them being collected.
    Although there are legitimate reasons and benefits of the 
collection and use of information online, we want to ensure 
that Americans understand how their information is being used. 
Specifically, how do companies use algorithms to make decisions 
and deliver content to consumers? What information goes into 
these complex algorithms, and how do they control the 
information that comes out? How important are human decisions 
in creating the algorithms and interpreting the results? Are 
the results of the researches we conduct online objective, or 
are companies controlling the information we get?
    These are all fair, legitimate questions that we intend to 
explore. It is our job to make sure consumers have the 
information they need to make informed decisions, especially 
when it comes to the flow of their personal information online. 
With that said, it is also important to understand how 
effective privacy policy disclosures are. Although some 
scholars believe such disclosures empower the consumers, others 
contend they are only there for the lawyers and are impossible 
to read. For that reason, we must consider whether there are 
more effective ways to empower the consumer.
    I would like to thank Chairman Blackburn for her commitment 
to these issues, and I look forward to exploring these complex 
but important issues with all stakeholders. Again, I want to 
thank our witnesses for being here today, and at this time I 
would like to recognize the gentlelady from Illinois, the 
ranking member of the subcommittee, for 5 minutes.
    [The prepared statement of Mr. Latta follows:]

               Prepared statement of Hon. Robert E. Latta

    Good morning, I'd like to welcome everyone back from the 
Thanksgiving holiday to our joint subcommittee hearing. I'd 
like to thank our witnesses for being here today. I would 
venture to guess many people were able to get a jump start on 
their holiday shopping. Early reports show online shopping 
revenues rose over 17 percent from last year, which makes our 
hearing this morning so timely.
    When Chairman Walden became chair of the Energy and 
Commerce Committee, we agreed that keeping our focus on the 
consumer was a priority for the committee. In everything the 
Digital Commerce and Consumer Protection subcommittee has 
done--whether it has been exploring new technologies through 
our Disrupter Series or the bipartisan work that went into the 
SELF DRIVE Act--our goal has always been to act in the best 
interest of the consumer and the American people.
    Earlier this fall, the Equifax data breach compromised the 
personal information of over 145 million Americans. This 
troubling incident raised many questions about credit industry 
practices with respect to the collection of consumer 
information. Many Americans--some of who had never heard of 
Equifax--were confused as to how their sensitive personal 
information could have been compromised by a company they had 
never interacted with.
    Just last week, Uber announced their systems were hacked 
exposing data on over 57 million users. Rather than alert 
authorities and make the breach known to their users and 
drivers--Uber kept the hack secret for a year. Disregard of the 
law and disregard of consumers and drivers trust all require 
close scrutiny. The Digital Commerce and Consumer Protection 
subcommittee will continue our work to protect consumers and 
make sure those who disregard the law are held accountable.
    As investigations continue, the importance of this hearing 
cannot be understated. Polls shows Americans both feel that 
technology has had a positive effect on our society, but are 
also skeptical about how their personal information is used by 
major technology companies. As policymakers, it is our 
obligation to ask the tough questions and make sure consumers 
understand how their information is being used in our digitally 
driven economy.
    That is why we will explore today how personal information 
about consumers is collected online and--importantly--how 
companies use that information to make decisions about the 
content consumers see.
    Right now, there are more than 224 million smartphone users 
in America and U.S. consumers spend about 5 hours a day on 
their mobile devices. As we continue to see the number of 
connected devices increase and our digital economy expand, 
Americans are only going to spend more and more time online--
browsing the web, shopping, or checking social media--with more 
information about them being collected.
    Although there are legitimate reasons and benefits to the 
collection and use of information online, we want to ensure 
that Americans understand how their information is being used.
    Specifically, how do companies use algorithms to make 
decisions and deliver content to consumers? What information 
goes into these complex algorithms and how do they control the 
information that comes out? How important are human decisions 
in creating the algorithms and interpreting their results? Are 
the results of the searches we conduct online objective or are 
companies controlling the information we get? These are all 
fair, legitimate questions that we intend to explore.
    It is our job to make sure consumers have the information 
they need to make informed decisions--especially when it comes 
to the flow of their personal information online. With that 
said, it is also important to understand how effective privacy 
policy disclosures are. Although some scholars believe such 
disclosures empower the consumer, others contend that they are 
only there for the lawyers and are impossible to read. For that 
reason, we must consider whether there are more effective ways 
to empower the consumer.
    I would like to thank Chairman Blackburn for her commitment 
to these issues, and I look forward to exploring these complex, 
but important issues with all stakeholders.
    Thank you again to our witnesses for being here today.

       OPENING STATEMENT OF HON. JANICE D. SCHAKOWSKY, A 
     REPRESENTATIVE IN CONGRESS FROM THE STATE OF ILLINOIS

    Ms. Schakowsky. Thank you, Mr. Chairman. We like to think 
of the internet as an open marketplace and forum for the 
exchange of ideas. In reality, the information that consumers 
see is determined in part by tech companies. Today, algorithms 
determine what appears in web ads, search results, and your 
customized news feed. Some of the content you are presented may 
be based on personal information such as your gender, race, and 
location. It may also depend on how much companies have paid to 
get that content in front of you.
    The internet and social media have changed how Americans 
consume news, information, and advertising. According to an 
August 2017 survey by the Pew Research Center, two-thirds of 
Americans get at least some of their news through social media. 
Consumers rely on a handful of popular platforms, making the 
algorithms of those platforms tremendously powerful.
    On a sinister level, organizations and even nation-states 
can exploit algorithms to spread disinformation, as we saw with 
Russian interference in the 2016 elections. In addition, 
platforms profit by selling ads targeted to specific groups 
based on their demographics and inferences made through their 
engagement with content on the platform. This may have some 
benefit: Consumers see ads that they are actually interested 
in. But the line between tailoring advertising and facilitating 
discrimination can get murky.
    As we grapple with algorithms on the internet, the Federal 
Communications Commission is considering big changes that would 
allow corporations to further shape what content consumers 
access. On December 14th, the FCC will vote on whether to undo 
the Open Internet Order, which protects net neutrality. If that 
proposal is adopted, internet service providers will be able to 
control consumers' access to content. They can make a website 
load faster or slower depending on whether the content provider 
pays for the better speed, or an ISP can block content 
altogether.
    Destroying that neutrality would change the internet as we 
know it, and how does a small business compete online if it now 
has to pay every ISP in the country for its website to load as 
fast as big corporation competitors? What happens to the 
exchange of ideas when access to some content is restricted? 
This is a disturbing amount of power that the FCC might cede to 
for-profit broadband providers.
    We already have examples of what broadband providers do 
when empowered to block content. Verizon blocked text messages 
from reproductive rights group NARAL, calling them, quote, 
controversial, unquote. AT&T limited use of FaceTime to 
incentivize its customers to purchase more expensive data 
plans. TELUS, another telecom company, blocked the website of a 
union with which it had a labor dispute. No wonder millions of 
internet users have filed comments in support of maintaining 
the Open Internet Order. Just since last Monday, my office has 
received about 500 calls from net neutrality supporters.
    Americans are watching the FCC's next move. The FCC under 
Chairman Pai is also encouraging consolidation and media 
ownership. It has bent over backward to clear the way for 
Sinclair Broadcast Group's acquisition of Tribune Media. 
Congress established a 39 percent cap on the national audience 
one broadcaster can cover, but Chairman Pai moved to reinstate 
the outdated UHF discount so that Sinclair can potentially 
cover 70 percent of the national audience. This media 
consolidation is a threat to local journalism, especially as 
Sinclair forces its stations to run nationally produced, quote, 
must-air, unquote, content.
    Big corporations are being given more and more influence 
over the information that Americans receive, from news feeds to 
websites, from smart phone to TVs. Congress and Federal 
watchdogs like the FCC have a responsibility to push back on 
corporate power when it threatens fair competition and free 
expression. I look forward to our witnesses' insights on how we 
fulfill that responsibility, and I yield back. Thank you.
    Mr. Latta. Thank you. The gentlelady yields back, and at 
this time the Chair recognizes the gentlelady from Tennessee, 
the chairman of the Communications and Technology Subcommittee, 
for 5 minutes.

OPENING STATEMENT OF HON. MARSHA BLACKBURN, A REPRESENTATIVE IN 
              CONGRESS FROM THE STATE OF TENNESSEE

    Mrs. Blackburn. Good morning, and welcome to all of our 
witnesses. I want to thank my colleague Mr. Latta for working 
closely with me and our committee to put together this stellar 
panel so that we can talk about all things virtual.
    Although we often refer to the world on the other side of 
the screen as the virtual world, we are seeing that, when 
things go wrong, the real-world impacts on our privacy, 
finances, knowledge base, and even freedom of expression are 
anything but virtual. They are very, very real. As so many of 
these issues overlap between our two subcommittees, I am 
pleased we are able to kick off our exploration of these issues 
as a team.
    On a number of fronts, we are seeing the pressure turned up 
on the tech companies that often serve as the new town squares 
for public discourse as governments and users are demanding 
that certain speech be shut down. Some of the responses have 
perhaps been a disappointment from the perspective of free 
speech. Companies that began as start-ups in Silicon Valley 
garages have fundamentally changed the way we communicate with 
one another about everything from the song we want to hear, to 
what stock to buy, to what is the best way to change our 
healthcare delivery system.
    These multinational corporations now respond to pressures 
that do not necessarily align with American values, so we need 
to examine how and why content is being blocked, filtered, or 
prioritized. This may all sound faintly similar to another 
topic, net neutrality. Exercise caution here, as it is 
important to note the FCC's current rules only apply to ISPs, 
not social media or search platforms.
    In some very concrete ways, the open internet is being 
threatened by certain content management practices. These 2-
year-old FCC rules have not and cannot address these threats, 
so it is disheartening to see Title II regulatory advocates 
happily conflating the two to divert attention from who is 
actually blocking content. The current FCC proposal to return 
internet regulation back to the bipartisan light-touch norm 
also reminds us that we are simply shifting authority back to 
the FTC to handle privacy matters.
    The previous head of the FCC swiped jurisdiction from the 
FTC, a 100-plus-year-old institution established by a 
Democratic President to act against trusts. As discussed at our 
previous hearings on the limits of the FCC, its authority can 
only touch one part of the internet ecosystem, and thus it 
ignores edge provider services that collect arguably more data 
than ISPs.
    As you may have heard, in order for consumers to be able to 
protect their virtual you, I introduced a bill that would 
create a level and fair privacy playing field by bringing all 
entities that collect and sell personal data of individuals 
under the same unified rules. Given the witnesses' testimony 
today, let me also plug another bipartisan initiative we have 
addressed: data security. Given the implications and risk 
associated with transferring all of this data, it is imperative 
that we address data security. It is a timely issue.
    I look forward to working with my friends across the aisle 
on this, data security, and on privacy, the BROWSER Act, and 
all of these topics so that we can settle our differences right 
here with legislative authority in these hearing rooms rather 
than relinquishing that authority to regulators in power. I 
thank the chairman for his collaboration and work on this 
issue, and I yield back the balance of my time.
    [The prepared statement of Mrs. Blackburn follows:]

              Prepared statement of Hon. Marsha Blackburn

    Good afternoon, and welcome to our witnesses. Let me also 
thank my colleague Mr. Latta for working closely with me to put 
together this all-star panel to discuss all things virtual. 
Although we often refer to the world on the other side of our 
screens as the virtual world, we are seeing that when things go 
wrong, the real world impacts on our privacy, finances, 
knowledge base, and even freedom of expression are anything but 
virtual. As so many of these issues overlap between our two 
subcommittees, I am pleased that we are able to kick off our 
exploration of them as a team.
    On a number of fronts, we are seeing the pressure turned up 
on the tech companies that often serve as the new town squares 
for our public discourse. As governments and users are 
demanding that certain speech be shut down, some of the 
responses have perhaps been a disappointment from the 
perspective of free speech. Companies that began as start-ups 
in Silicon Valley garages have fundamentally changed the way we 
communicate with each other about everything from what song we 
want to hear, to what stock we want to buy or sell, to what is 
the best way to change our health care system. These 
multinational corporations now respond to pressures that do not 
necessarily line up with American values, so we need to examine 
how and why content is being blocked, filtered, or prioritized.
    This may all sound faintly similar to another hot topic--
net neutrality. Exercise caution here as it is important that 
we note: the FCC's current rules only apply to ISPs, not social 
media or search platforms. In some very concrete ways, the open 
internet is being threatened by certain content management 
practices. These 2-year-old FCC rules have not and cannot 
address these threats, so it is disheartening to see Title 2 
regulatory advocates happily conflating the two to divert 
attention from who is actually blocking content.
    The current FCC proposal to return internet regulation back 
to the bipartisan light-touch norm also reminds us that we are 
simply shifting authority back to the FTC to handle privacy 
matters. The previous head of the FCC swiped jurisdiction from 
the FTC, a 100-plus-year-old institution established by a 
Democratic president to act against trusts. As discussed at our 
previous hearings on the limits of the FCC, its authority can 
only touch one part of the internet, ecosystem and thus it 
ignores edge provider services that collect arguably more data 
than ISPs. As you may have heard, I introduced a bill that 
would create a level and fair privacy playing field by bringing 
all entities that collect and sell the personal data of 
individuals under the same rules.
    Given the witnesses testimony today, let me also plug 
another bipartisan initiative I have worked on in the past--
data security. Given the implications and risks associated with 
transferring all of this data, it feels rather timely. I look 
forward to working with my friends across the aisle on this and 
all of these topics so we settle differences in this hearing 
room as opposed to relinquishing our authority to regulators in 
power.

    Mr. Latta. Thank you. The gentlelady yields back. And, 
before I recognize our next Member, I just want to mention to 
our witnesses we have another subcommittee that is going on 
right now, so you will have Members coming in and out of 
subcommittee today. And at this time, the Chair recognizes the 
gentleman from Pennsylvania, the ranking member on C&T, for 5 
minutes.

OPENING STATEMENT OF HON. MICHAEL F. DOYLE, A REPRESENTATIVE IN 
         CONGRESS FROM THE COMMONWEALTH OF PENNSYLVANIA

    Mr. Doyle. Thank you, Mr. Chairman, for holding this joint 
hearing, and thank you to the witnesses who have come before us 
today.
    Machine learning and artificial intelligence are powerful 
tools that are reshaping our country and our economy. In places 
like my hometown of Pittsburgh, our leadership in artificial 
intelligence is leading to new technologies and new advances 
that have the potential for revolutionary changes. I hope this 
committee can continue to investigate and understand this 
important technology and the impacts that it will have.
    That being said, troubling recent events such as the hack 
of Equifax continue to show light on the dark world of data 
brokers and data mining. Credit rating agencies play a central 
role in many Americans' lives whether you are buying a home, a 
car, or even a new phone. Your ability to demonstrate good 
credit in the eyes of these institutions is tantamount to being 
allowed to make a purchase or being told that you do not pass 
Go. Americans have little recourse, and our Government provides 
little oversight of these institutions and their practices. 
They are increasingly using big data and machine learning to 
make judgments about individuals and their ability to access 
and use credit.
    Data breaches at these companies pose grave threats to 
nearly every American, and I think this warrants further 
investigation. However, today I am deeply concerned that this 
hearing is happening in the shadow of the FCC's efforts to end 
network neutrality and this Congress' own decision to use the 
Congressional Review Act on the FCC's broadband privacy rules. 
These policies are and were robust protections for consumers 
that are at the heart of our discussions here today.
    In addition, Ms. Moy's testimony refers in numerous places 
to the CRA against rules requiring mandatory arbitration by 
financial institutions. The majority does not seem content to 
merely strip Americans of their legal and regulatory 
protections. They are going even further now and working to 
deny them their access to the courts, as well. The majority 
seems willing only to give lip service to these real consumer 
protections that they have already cast aside.
    The FCC's current efforts to repeal the Open Internet Order 
and end network neutrality are a perfect case in point. The 
need for net neutrality was borne out of a long history of 
anti-consumer and anti-competitive behavior that limited 
consumers' access to content and information, new technologies, 
and competitive choices. ISPs have blocked consumer access to 
services that compete with their own services, new services, 
and transformative services more times than I can count. The 
FCC's privacy rules themselves were a reaction to bad behavior 
by the ISPs.
    For years, ISPs have taken actions to track user behavior 
online using deep packet inspection, undeletable supercookies, 
and even force consumers to pay them on top of the sky-high 
fees they already charge to retain their privacy. Consumers 
were protected from these abusive practices until Congress and 
President Trump recklessly acted to nullify these rules.
    I cannot reiterate to my colleagues enough that when you 
own the pipe to the home, you own access to the consumer, as 
ISPs have demonstrated so many times. Repealing these rules 
will have grave consequences on consumers and the vibrance of 
the online ecosystem. I continue to urge Chairman Pai to end 
his quixotic misadventure, and with that being said, I will 
yield the remainder of my time to Mr. McNerney.
    Mr. McNerney. I thank the ranking member. While I am glad 
we are holding today's hearing about protecting online 
consumers, I am disappointed that the Republicans on this 
committee and at the Federal Communications Commission are 
doing just the opposite. Earlier this year, Republicans passed 
the privacy CRA, eliminating broadband privacy protections for 
consumers' personal information.
    In response, I introduced the MY DATA Act. This legislation 
would give the Federal Trade Commission rulemaking and 
enforcement authority so that consumers can have strong privacy 
and data security protections across the internet. Not a single 
Republican agreed to cosponsor this bill. In addition, this 
December, the FCC is expected to adopt Chairman Pai's proposal 
to dismantle net neutrality.
    Thousands of constituents have reached out to my office 
this year to express concerns about eliminating broadband 
privacy and net neutrality protections. I urge my Republican 
colleagues to take actions to actually protect consumers 
instead of talking about protecting consumers while exposing 
consumers to online mischief. I yield back.
    Mr. Latta. Thank you very much. The gentleman yields back. 
And at this time, the Chair now recognizes the gentleman from 
Oregon, the chairman of the full committee, for 5 minutes.

  OPENING STATEMENT OF HON. GREG WALDEN, A REPRESENTATIVE IN 
               CONGRESS FROM THE STATE OF OREGON

    Mr. Walden. Thank you, Mr. Latta, and good morning, 
everyone. Thanks for being here, especially thanks to our 
witnesses.
    And today we begin a critical discussion about the 
evolution of consumers' online environment. We will dive into 
many of the important questions surrounding the future of data 
access and content management in a marketplace driven by 
algorithms. Just in the past decade, the internet economy has 
grown, thrived, and evolved, as you all know, substantially. It 
is amazing what is happening there.
    The smart phones we carry with us everywhere, the tablets 
we log on to, the smart home devices in our kitchens, all 
represent a transformational shift in how Americans gather 
information, receive their news and content, and how they 
connect with friends and with family. These services are 
convenient, efficient, and provide valuable and tangible 
benefits to American consumers.
    The companies behind the services have created thousands 
and thousands of jobs and brought the U.S. into the forefront 
of technology and innovation. In exchange for using certain 
websites or platforms, consumers are willing to share personal 
details about themselves--names, locations, interests, and 
more. The context of the relationship drives that exchange.
    Now, depending on the service, tech companies and online 
platforms make their money because they know who you are, where 
you are, what you like, what photos and videos you take and 
watch, and what news you read. The depth and power of data will 
be supercharged with the proliferation of connected and 
embedded devices in the Internet of Things. Billions of IoT 
devices will surely be deployed, linking machines to other 
machines and transmitting massive amounts of data and 
information to connect Americans to even more services, 
conveniences, and benefits from all around the globe.
    So what is behind these services and activities? Algorithms 
and data. Algorithms are a sequence of instructions to solve a 
problem or complete a task. These instructions help devices and 
apps predict user preferences as well as provide the content 
and advertising you see in your social media feed. Data serve 
as inputs or signals to those algorithms. Well-intentioned 
algorithms can lead to unanticipated consequences. For example, 
algorithmic bots are being profusely designed to steal or to 
cheat in online gambling and ticket sales.
    Humans remain a critical part of the creation and 
monitoring of these systems. In recent months, reports of data 
breaches and algorithms gone awry have demonstrated the 
potentially negative influences of digital technology on 
Americans' lives. This committee has done extensive work on 
issues surrounding consumer protection and data breaches. We 
brought in the former CEO of Equifax for a hearing, and we 
continue to push for answers on behalf of American consumers.
    At the same time, there have been some high-profile 
instances of major social media platforms blocking content for 
questionable reasons using opaque processes. As a result of all 
of this, consumers are concerned about whether they can trust 
online firms with the integrity of news and information they 
disseminate, the welfare of its users, and on a much larger 
scale the preservation of our own democratic institutions. All 
these are part of the big public discussion going on right now.
    As we all know net neutrality is the issue of the moment, 
but regardless of where you stand on that policy, the recent 
attacks on Chairman Pai and particularly his children are 
completely unacceptable and have no place in this debate. 
Period. I condemn it in the strongest terms, and I call on the 
entire tech community and my colleagues on both sides of the 
aisle to condemn it, as well.
    In light of the current controversy surrounding net 
neutrality rules for ISPs, it is important to examine how 
content is actually being blocked or promoted or throttled 
every day on the internet and not by the ISPs. Net neutrality 
rules do not address the threats to the open internet that we 
will discuss today.
    Now, the goal for today's hearing is to help provide all 
Americans with a better understanding of how their data flows 
online, how online platforms and online media sources determine 
what they see or don't see, and the extent of and methods by 
which their information is collected and used by online firms. 
Americans should be able to feel confident that their well-
being, freedom of expression, and access to the content of 
their choice are not being wholly sacrificed for profit.
    Americans should have vibrant, competitive markets both 
offline and online where consumers know their rights and 
options and have the freedom to choose what is best for their 
circumstances. It is undeniable the internet has created 
millions of new jobs, tremendous opportunities, access in ways 
unimaginable just a few years ago, but it has also created 
these new risks and challenges.
    So, in the name of convenience, is there a potential for 
online firms to undermine America's privacy and security in a 
way that they don't expect or know about? Are the current 
policies regarding the collection and use of personal data 
working? Are consumers harmed by this hyperpersonalization? And 
finally, are firms' content management practices constraining 
America's ability to speak and to listen freely on an open 
internet?
    Consumers should remain as safe from unfair, deceptive, and 
malicious practices by online firms and their algorithms on the 
internet as they do in the real world. And we are here today to 
dig into these tough questions, and we appreciate your advice 
and counsel from our witnesses today. And with that, Mr. Chair, 
I yield back.
    [The prepared statement of Mr. Walden follows:]

                 Prepared statement of Hon. Greg Walden

    Good morning. Today we begin a critical discussion about 
the evolution of consumers' online environment. We will dive 
into many important questions surrounding the future of data 
access and content management in a marketplace driven by 
algorithms.
    Just in the past decade, the internet economy has grown, 
thrived, and evolved substantially. The smartphones we carry 
with us everywhere, the tablets we log on to, and the smart 
home devices in our kitchens all represent a transformational 
shift in how Americans gather information, receive news and 
content, and connect with friends and family.
    These services are convenient, efficient, and provide value 
and tangible benefits to American consumers. The companies 
behind the services have created jobs, and brought the U.S. 
into the forefront of technological innovation.
    In exchange for using certain websites or platforms, 
consumers are willing to share personal details about 
themselves--names, locations, interests, and more. The context 
of the relationship drives that exchange.
    Depending on the service, tech companies and online 
platforms make their money because they know who you are, where 
you are, what you like, what photos and videos you take and 
watch, and what news you read.
    The depth and power of data will be supercharged with the 
proliferation of connected and embedded devices in the Internet 
of Things.
    Billions of IoT devices will surely be deployed, linking 
machines to other machines, and transmitting massive amounts of 
data and information to connect Americans to even more 
services, conveniences and benefits from all around the globe.
    What's behind these services and activities? Algorithms and 
data.
    Algorithms are a sequence of instructions to solve a 
problem or complete a task. These instructions help devices and 
apps predict user preferences as well as provide the content 
and advertising you see in your social media feed. Data serve 
as inputs or signals to the algorithms.
    Well-intentioned algorithms can lead to unanticipated 
consequences. For example, algorithmic bots are being 
purposefully designed to steal or to cheat in online gambling 
and tickets sales. Humans remain a critical part of the 
creation and monitoring of these systems.
    In recent months, reports of data breaches and algorithms 
gone awry have demonstrated the potentially negative influences 
of digital technology on Americans' lives.
    This committee has done extensive work on issues 
surrounding consumer protection and data breaches--we brought 
in the former CEO of Equifax for a hearing--and we continue to 
push for answers on behalf of consumers.
    At the same time, there have been some high-profile 
instances of major social media platforms blocking content for 
questionable reasons, using opaque processes.
    As a result of all this, consumers are concerned whether 
they can trust online firms with the integrity of the news and 
information they disseminate, the welfare of its users, and, on 
a much larger scale, the preservation of our democratic 
institutions.
    As we all know, net neutrality is the issue of the moment, 
but regardless of your position on the policy, the recent 
attacks on Chairman Pai and particularly his children, are 
completely unacceptable and have no place in this debate. I 
condemn it in the strongest terms and I call on the entire tech 
community and my colleagues on both sides of the aisle to 
condemn it as well.
    In light of the current controversy surrounding net 
neutrality rules for ISPs, it's important to examine how 
content is actually being blocked and throttled every day on 
the internet--and not by the ISPs.
    While I will continue to pursue legislation on net 
neutrality rules, the fact is, they do not and cannot address 
the threats to the open internet that we will discuss today.
    The goal for today's hearing is to help provide all 
Americans with a better understanding of how their data flows 
online, how online platforms and online media sources determine 
what they see or don't see, and the extent of and methods by 
which their information is collected and used by online firms.
    Americans should be able to feel confident that their well-
being, freedom of expression, and access to the content of 
their choice are not being wholly sacrificed for profit.
    Americans should have vibrant, competitive markets both 
offline and online, where consumers know their rights and 
options, and have the freedom to choose what is best for their 
circumstances.
    It is undeniable the internet has created new jobs, 
tremendous opportunity, and access in ways unimaginable just a 
few years ago. But it has also created new risks and 
challenges.
    In the name of convenience, is there the potential for 
online firms to undermine Americans' privacy and security in a 
way that they don't expect?
    Are the current policies regarding the collection and use 
of personal data working? Are consumers harmed by this hyper-
personalization?
    And finally, how are firms' content management practices 
constraining Americans' ability to speak and to listen freely 
on an open internet?

    Mr. Latta. Thank you very much. The gentleman yields back, 
and at this time the Chair recognizes the gentleman from New 
Jersey, the ranking member of the full committee, for 5 
minutes.

OPENING STATEMENT OF HON. FRANK PALLONE, JR., A REPRESENTATIVE 
            IN CONGRESS FROM THE STATE OF NEW JERSEY

    Mr. Pallone. Thank you, Mr. Chairman. The internet is home 
to some of the most important conversations taking place today. 
As internet companies find ways for Americans to communicate, 
our democracy should be stronger than ever, but as you all know 
something else is going on. Our national dialogue is being 
curated by companies policing content, and the number of 
websites handling this traffic has consolidated to just a few 
key players.
    The aim of internet platforms is monetizing web traffic, 
not public policy. Algorithms created for the purpose of 
increasing ad clicks is what ends up shaping what we see 
online, and too often this content is not an accurate 
reflection of the real world. Structural flaws built into the 
algorithms used to sort online content may result in racial and 
other bias in our news feeds.
    As diverse voices are squeezed out, bias increases even 
further, and this is simply not acceptable, and I look forward 
to hearing more today about what we can do about it. 
Unfortunately, forces are at work here in Washington that make 
this problem worse. At every turn, we see efforts to give more 
power to gatekeepers, either by eviscerating net neutrality and 
privacy or by picking favorite voices for preferred regulatory 
treatment.
    Even now, as we hold a hearing to talk about mitigating 
bias on the internet, FCC Chairman Pai is planning to introduce 
more bias into the system. The net neutrality rules that he 
plans to destroy are the protections that ensure that we the 
people can decide for ourselves what we do and say online, and 
Chairman Pai's plan will fundamentally change the free and open 
internet as we know it. Independent voices, those outside the 
mainstream, may be most at risk simply because they don't have 
an affiliation with the companies that run the internet.
    Unfortunately, broadband companies have more than just 
financial reasons to obstruct access to independent content, it 
can also be political. Under Chairman Pai's plan, nothing stops 
those in power from pushing broadband companies to censor 
dissenting voices or unpopular opinions or to promote views 
that they support. We are seeing more and more often how this 
administration is using its political might to pressure even 
large companies.
    And this is not a partisan point or even a political one. 
Jeopardizing the national dialogue should concern all of us. 
The dialogue that happens online is critical for our democracy. 
Chairman Pai's move comes after this Congress acted earlier 
this year to wipe out privacy and data security online. Under 
President Obama, the FCC adopted fair rules to protect the 
little guy: ask before collecting information, don't share it 
without consent, and take reasonable measures to safeguard it. 
But that was too much for congressional Republicans who voted 
to take away these protections and hand over consumers' data to 
big business.
    Sadly, there is still more to come. Over this past year, 
the FCC has taken every step possible to ensure that Sinclair 
broadcasting, already the largest owner of broadcast stations 
in the country, becomes even bigger. And these steps by the FCC 
fly in the face of laws Congress put in place to protect local 
voices. We understand that diverse perspectives are critical 
for our communities and strengthen our democracy. Instead, the 
FCC is doing everything it can to allow one company to control 
what people hear no matter where they are in the country and 
that is simply not what we intended.
    So I look forward to discussing ways to eliminate bias in 
our communication systems. We need to figure out how to wrest 
power over information from corporations and return it back to 
the people. And I yield the remainder of my time to the 
gentlewoman from New York, Ms. Clarke.
    [The prepared statement of Mr. Pallone follows:]

             Prepared statement of Hon. Frank Pallone, Jr.

    The internet is home to some of the most important 
conversations taking place today. As internet companies find 
ways for Americans to communicate, our democracy should be 
stronger than ever. But as we all know, something else is going 
on. Our national dialogue is being curated by companies 
policing content, and the number of websites handling this 
traffic has consolidated to just a few key players.
    The aim of internet platforms is monetizing web traffic, 
not public policy. Algorithms created for the purpose of 
increasing ad clicks is what ends up shaping what we see online 
and too often, this content is not an accurate reflection of 
the real world. Structural flaws built into the algorithms used 
to sort online content may result in racial and other bias in 
our news feeds. As diverse voices are squeezed out, bias 
increases even further. This is simply not acceptable and I 
look forward to hearing more today about what we can do about 
it.
    Unfortunately, forces are at work here in Washington to 
make this problem worse. At every turn, we see efforts to give 
more power to gatekeepers either by eviscerating net neutrality 
and privacy or by picking favorite voices for preferred 
regulatory treatment.
    Even now, as we hold a hearing to talk about mitigating 
bias on the internet, FCC Chairman Pai is planning to introduce 
more bias into the system. The net neutrality rules that he 
plans to destroy are the protections that ensure that we, the 
people, can decide for ourselves what we do and say online. 
Chairman Pai's plan will fundamentally change the free and open 
internet as we know it.
    Independent voices--those outside the mainstream--may be 
most at risk simply because they don't have an affiliation with 
the companies that run the internet.
    Unfortunately, broadband companies have more than just 
financial reasons to obstruct access to independent content--it 
can also be political. Under Chairman Pai's plan, nothing stops 
those in power from pushing broadband companies to censor 
dissenting voices or unpopular opinions or to promote views 
they support. We are seeing more and more often how this 
administration is using its political might to pressure even 
large companies.
    This is not a partisan point or even a political one. 
Jeopardizing the national dialogue should concern all of us. 
The dialogue that happens online is critical for our democracy.
    Chairman Pai's move comes after this Congress acted earlier 
this year to wipe out our privacy and data security online. 
Under President Obama, the FCC adopted fair rules to protect 
the little guy--ask before collecting information, don't share 
it without consent, and take reasonable measures to safeguard 
it. But that was too much for Congressional Republicans, who 
voted to take away these protections and hand over consumers' 
data to big business.
    Sadly, there is still more to come. Over this past year, 
the FCC has taken every step possible to ensure that Sinclair 
Broadcasting--already the largest owner of broadcast stations 
in the country--becomes even bigger.
    These steps by the FCC fly in the face of the laws Congress 
put in place to protect local voices. We understand that 
diverse perspectives are critical for our communities and 
strengthen our democracy. Instead, the FCC is doing everything 
it can to allow one company to control what people hear no 
matter where they are in the country. That is simply not what 
we intended.
    So I look forward to discussing ways to eliminate bias in 
our communications systems. We need to figure out how to wrest 
power over information from corporations and return it back to 
the people.
    Thank you, I yield back.

    Ms. Clarke. I thank you, Mr. Ranking Member Pallone, for 
yielding me time. Today's hearing is of great importance to me 
for various reasons, both as a congresswoman and as a consumer. 
You see, technology continues to touch all areas of our lives. 
and its reach will continue to grow in the coming days, weeks, 
months, and years.
    With greater reach comes greater responsibility. Companies 
must ensure that the algorithms used for their services and 
products are free from all biases. including racial, ethnic, 
gender, sexual orientation biases. That includes making sure 
there is a diverse employee base behind the scenes ensuring 
these algorithms accurately represent American consumers.
    As a member of the Congressional Black Caucus, I would like 
to highlight the great work of the CBC Diversity Task Force and 
the CBC TECH 2020 initiative, two entities that have been doing 
a substantive deep-dive analysis into the progress of the 
American tech sector in accomplishing meaningful diversity and 
inclusion in the technology space.
    Additionally, I would like unanimous consent to submit for 
the record a letter my colleagues, Representatives Butterfield, 
Cleaver, and Kelly, and myself sent to Facebook regarding their 
site's use of ethnic affinity search criteria, which allow 
users to violate the Fair Housing Act. This is just an example 
of abuse within the algorithm space that really needs to be 
monitored and addressed, and I hope that we will get some 
recommendations from you here today.
    It is my understanding that this is being addressed in the 
short term through Facebook. I just want to go on the record 
that this is a concern to my colleagues and I. These issues are 
vitally important, and I look forward to today's testimony. Mr. 
Chairman, I yield back.
    Mr. Latta. And without objection, the letter is accepted 
for the record.
    [The information appears at the conclusion of the hearing.]
    Mr. Latta. And the gentlelady yields back. This concludes 
the Member opening statements. The Chair reminds Members that, 
pursuant to the committee rules, all Members' opening 
statements will be made part of the record. Additionally, I ask 
unanimous consent that Energy and Commerce members not on the 
Subcommittee on Digital Commerce and Consumer Protection or the 
Subcommittee on Communications and Technology be permitted to 
participate in today's hearing. Without objection, so ordered.
    Again, I want to thank our witnesses for being with us 
today, because it is very important for us to hear from you and 
being here to testify before the subcommittee. Today's 
witnesses will have the opportunity to give 5-minute opening 
statements followed by a round of questions from our Members.
    Our witness panel for today's hearing will include Dr. Omri 
Ben-Shahar, the Leo and Eileen Herzel Professor of Law at the 
University Chicago of Law; Ms. Kate Klonick, the resident 
fellow for the Information Society Project at Yale Law School; 
Ms. Laura Moy, the deputy director of the Georgetown Law Center 
on Privacy and Technology; Dr. Catherine Tucker, the Sloane 
Distinguished Professor of Management and Science and Professor 
of Marketing at the MIT Sloane School of Management; Mr. Frank 
Pasquale, the Professor of Law at the University of Maryland, 
Francis King Carey School of Law; and Dr. Michael Kearns, the 
Professor and National Center Chair of the Department of 
Computer and Information Science at the University of 
Pennsylvania.
    Again I want to thank all of our witnesses for being with 
us today, and again you each have 5 minutes. If you will, just 
pull that mic up close and turn on the button. We look forward 
to hearing your testimony.
    And Doctor, we will start with you this morning. Thank you.

 STATEMENTS OF OMRI BEN-SHAHAR, PH.D., LEO HERZEL PROFESSOR IN 
 LAW, UNIVERSITY OF CHICAGO LAW SCHOOL; KATE KLONICK, RESIDENT 
  FELLOW, INFORMATION SOCIETY PROJECT, YALE LAW SCHOOL; LAURA 
    MOY, DEPUTY DIRECTOR, CENTER ON PRIVACY & TECHNOLOGY AT 
 GEORGETOWN LAW; CATHERINE TUCKER, PH.D., SLOANE DISTINGUISHED 
     PROFESSOR OF MANAGEMENT SCIENCE, MIT SLOANE SCHOOL OF 
  MANAGEMENT; FRANK PASQUALE, PROFESSOR OF LAW, UNIVERSITY OF 
MARYLAND; AND, MICHAEL KEARNS, PH.D., COMPUTER AND INFORMATION 
         SCIENCE PROFESSOR, UNIVERSITY OF PENNSYLVANIA

                  STATEMENT OF OMRI BEN-SHAHAR

    Dr. Ben-Shahar. Thank you, Chairman Latta. Thank you, 
Chairman Blackburn, for inviting me, Ranking Members Schakowsky 
and Doyle and members of the subcommittee, I cherish this 
opportunity to participate in the conversation.
    I am a law professor at the University of Chicago, and I 
specialize in consumer law and consumer protection. You will 
hear today a lot about the dangers of big data enterprise, how 
websites know our locations, how smart alarms know and predict 
our vacations, how employers and insurers know our medications, 
and even Fitbit records our dedication.
    We of course all know the data-driven economy delivers 
enormous convenience and benefits too by offering personalized 
experience to consumers, but concerns about discrimination, 
manipulation, data security, and market power and the potential 
harms they might cause ought to be taken seriously. Still, it 
is important throughout this inquiry that the basic question--
What is the consumer injury?--be answered before we begin 
thinking about what the solution ought to be.
    You will probably hear today other speakers call for more 
transparency on how data is used and secured so as to give 
consumers more control over their data and allow them to make 
more informed decisions. Chairman Walden invited such noble 
proposals of transparency, writing eloquently in an op-ed, 
quote, ``It is our job to shine the light on these practices 
for consumers and ensure transparency in the marketplace so 
that they can make informed choices.''
    I would like to spend my remaining 4 minutes or so to try 
to talk you out of this transparency instinct. It is not that I 
don't like transparency or informed decision, it is just that 
this technique has never worked in any area, and it is 
decisively unlikely to yield any benefit here. I co-authored a 
book titled ``More Than You Wanted To Know,'' in which I looked 
at the effect of transparency laws. These are the numerous laws 
that require companies to give consumers full disclosures to 
help consumers make informed choices.
    Mandated disclosure is probably the most common and for 
sure the least successful regulatory technique in American law. 
Disclosure requirements, we sometimes call them sunshine laws, 
have been used for decades as the primary tool for consumer 
protection to protect borrowers, investors, medical patients, 
internet users, insurance buyers, home buyers, in every area of 
the law, and the record confirmed by mountains of empirical 
evidence is abysmal--transparency doesn't make a difference.
    Transparency requires that companies give consumers 
disclosures, but consumers are not cooperating. They are not 
reading or using the disclosures. How could they? The texts are 
too long and cluttered.
    [Photo shown.]
    Here is a picture of a typical artifact of transparency, 
Apple's terms and conditions that include their privacy policy, 
which I printed out and assembled into a 30-foot scroll, 8-
point font, mind you, and hung from the top of the atrium at 
the University of Chicago Law School.
    Shoving this monstrosity in front of consumers: Is that 
what consumer protection ought to do? If consumers tried to 
read the disclosures, they would of course not understand them 
and would not be able to put them to profitable use. To use 
complex information, one needs experience and expertise which 
people simply do not have. Transparency is defeated not because 
it is a bad idea but because it is so overused.
    When you close a mortgage, you receive at least 50 
different disclosures so that you, quote, ``know before you 
owe.'' When you walk into a clinic or buy a product or enter a 
website or download an app or eat at a restaurant or check your 
bank balance, you receive disclosures, all in the name of 
transparency. Consumers have long become numb and indifferent.
    Any transparency effort in the area of data protection 
would meet the same consumer apathy. Do you really want to be 
the authors of an irrelevant policy? Can transparency be done 
more effectively? If disclosures are defeated by complexity, 
can simplicity save them? Simplification seems like an obvious 
solution: If disclosures are too long, shorten them; if too 
technical, use plain language; if poorly presented, improve the 
formatting. Unfortunately, simplification strategies have been 
tried for as long as disclosures have failed.
    In my research, I tested whether people who are sharing 
deeply private information with websites that engage in nasty 
data practices can be prompted to act more prudently by well-
designed privacy warnings. I discovered that no matter how 
simple, conspicuous, and alarming the warning the consumers 
receive, their behavior is entirely unchanged. Consumers don't 
pay attention to any of the transparency tools lavished upon 
them.
    To conclude, if Members of Congress believe that collection 
of consumers' data poses risks that require regulatory 
intervention, I advise that they look for solutions that are 
outside the popular but unsuccessful repertoire of mandated 
disclosure and transparency. Thank you.
    [The prepared statement of Dr. Ben-Shahar follows:]

[GRAPHIC(S) NOT AVAILABLE IN TIFF FORMAT]

    
    Mr. Latta. Thank you very much for your testimony this 
morning.
    And, Ms. Klonick, you are recognized for 5 minutes.

                   STATEMENT OF KATE KLONICK

    Ms. Klonick. Thank you. Chairmen Blackburn and Latta, 
Ranking Members Doyle and Schakowsky, and members of the 
subcommittees, thank you for having me here to discuss this 
important topic.
    Every day millions of people around the world post videos, 
pictures, and text to online speech platforms, but not 
everything that is posted remains there. Sites like Facebook, 
Twitter, and YouTube actively curate the content that is posted 
by their users through a mix of algorithmic and human processes 
broadly termed content moderation. Until recently, how and why 
these platforms made these decisions on users' speech was 
largely opaque.
    Over the last 2 years, I have interviewed dozens of former 
and current executives at these platforms as well as content 
moderation workers at these companies working abroad in an 
effort to better understand how and why these platforms 
regulate content. A summary of that research and my conclusions 
are the subject of my paper, ``The New Governors: The People, 
Rules, and Processes Governing Online Speech,'' forthcoming in 
the Harvard Law Review. My testimony today draws from that 
expertise and knowledge that I gained in researching and 
writing that article.
    As a threshold matter, when I refer to content moderation I 
am referring specifically and exclusively to the experience of 
the user in posting speech to a platform and what happens to 
that posted content in terms of removal or nonremoval. I am not 
speaking to the algorithm that configures the prioritization, 
promotion, order, or frequency of how content later appears in 
users' news feeds or Twitter feeds.
    And in that context, content moderation happens at many 
levels. It can happen before content is actually published on 
the site, and when a user uploads a photo, a message appears: 
``Upload completed. The video in your post is being processed. 
We will send you a note when it is ready for review.'' And the 
moderation process that happens in this moment between upload 
and publication largely runs through an algorithm screening 
that checks for matches in pixel fingerprints between illegal 
or banned content and the uploaded content. Examples of this 
include photo DNA for child pornography and content ID for 
copyrighted information.
    Only a very small amount of material is removed through 
these types of processes, and most is published, and once 
published it can be removed in two ways. The first is by 
platforms proactively using their own moderators, but because 
of the absolutely enormous amount of posts, this is not a 
feasible method for all but a very select area of moderation, 
such as extremist and terrorist content.
    The second way content is removed after publication is also 
how the vast majority of content is removed, through being 
flagged as violating community standards by other users on the 
site. After a piece of content is flagged, it will stay up, but 
a crop screen grab of the content is placed in a database 
queue, where it is eventually reviewed by trained human 
decision makers. They will look at the offending content and 
see if it actually violates the terms of service.
    With that background, I would like to use my brief time to 
clarify four major misconceptions about content moderation. 
First, that, contrary to this hearing's title, the vast 
majority of content moderation of user content is done by 
trained human decision makers who review content only after it 
has been flagged by other users and not by algorithms or AI or 
photo recognition.
    Second, while users who use sites like Facebook are given a 
public set of community standards guiding what kinds of content 
is posted by the site, a separate and much more detailed and 
much more regularly updated set of internal rules is used by 
human moderators in making their decisions. These internal 
rules at these companies are not currently known to the public.
    Third, Facebook and most platforms use one global set of 
rules with exceptions to comply with the laws of a given 
jurisdiction to curate content. This means, for example, the 
definitions of inappropriate sexual activity are the same for 
users in Canada as they are for users in India as they are for 
users in France.
    Finally, it is critical to note that the ability for these 
platforms to create this intricate system of governance to 
regulate content stems from incentives put in place by 
Communications Decency Act Section 230 which granted platforms 
immunity from intermediary liability in an effort to encourage 
sites to remove offensive content while also protecting against 
collateral censorship of users' speech.
    In many ways these platforms' self-regulation have very 
well met the goals of Section 230, but as access to online 
speech platforms has increasingly become an essential public 
right, new concerns about regulating platforms are being 
raised. While these and other concerns are undoubtedly present, 
changes to Section 230 or new regulations that might affect it 
should be considered with extreme caution and with a full 
appreciation of the potential damage that could be caused to 
consumer rights and to free speech. Thank you.
    [The prepared statement of Ms. Klonick follows:]

[GRAPHIC(S) NOT AVAILABLE IN TIFF FORMAT]

    
    Mr. Latta. And, again, thank you for your testimony.
    Ms. Moy, you are recognized for 5 minutes.

                     STATEMENT OF LAURA MOY

    Ms. Moy. Thank you. Good morning, Chairmen Blackburn and 
Latta, Ranking Members Doyle and Schakowsky, and distinguished 
members of the subcommittees.
    Consumers are frustrated. Ninety-one percent of adults feel 
that consumers have lost control of their personal information 
and nearly 70 percent think the law should do a better job of 
protecting their information. The law can do better, and it 
should do better. Consumers are in greatest need of greater 
control when they do not have a choice about whether to share 
the information in the first place. This is one reason that we 
have specific privacy laws that protect things like the 
information students share with educational institutions or the 
information patients share with doctors.
    In these contexts and others, it is not permissible for 
companies to simply do what they wish with consumer information 
as long as they are transparent about it, something we see all 
too often online; rather, strong privacy protections apply by 
default. We need similar protection by default in other 
situations where information sharing is unavoidable, as well--
for example, when consumer information is shared with a credit 
agency like Equifax or when consumer information is shared with 
the provider of an essential communication service like a 
broadband provider. We may also need protection by default for 
other types of online actors such as content platforms as they 
become bigger and more powerful and consumers increasingly find 
it unavoidable to share their information with those actors, as 
well. This is certainly a conversation worth having.
    But whatever specific information-sharing problem or 
problems Congress decides to address, it should keep a few 
things in mind. First, Congress should not eliminate existing 
protections for consumers' information. This really should go 
without saying, but unfortunately, in an incredibly unpopular 
move earlier this year, Congress voted to eliminate strong 
Federal privacy rules that would have applied to broadband 
access providers.
    Similarly, Congress has occasionally considered legislative 
proposals on data security and breach notification that would 
eliminate stronger State laws, but consumers want more 
protection for their information, not less. If Congress wishes 
to improve on the privacy and data security status quo, it 
should start by preserving the protections we already have. And 
just to touch for a second on net neutrality, the same applies 
in that context, as well.
    Today's hearing is surfacing some concerns about the power 
platforms have to editorialize the things internet users read 
and say, but at the same time the FCC is considering wholesale 
elimination of rules that prevent broadband providers from 
doing that. Just imagine how much worse things could get if we 
start allowing broadband providers to muck with content. Again, 
consumers in this area need more protection, not less.
    Second, prospective rulemaking authority is an incredibly 
important consumer protection tool. After-the-fact enforcement 
can be helpful, but an enforcement-only regime does not always 
create clarity, and because it comes only after a problem has 
occurred, it does not necessarily protect consumers from the 
problem in the first place.
    Granting rulemaking authority to an expert agency also 
fosters much-needed regulatory flexibility. We do not always 
know what the next privacy or data security threat will be, but 
unfortunately we all know that there will be one. An agency 
with rulemaking authority can respond to shifting threats more 
quickly than Congress.
     Third, consumer protections are only as good as their 
enforcement, so any protections Congress creates on privacy or 
data security must be accompanied by strong enforcement 
authority. Right now, the FTC does substantial work on privacy 
and data security, but with few exceptions it does not have the 
ability to seek civil penalties for privacy and data security 
violations. In fact, FTC staff and commissioners have appeared 
before Congress requesting civil penalty authority to buttress 
their ability to enforce. Agencies that are tasked with 
protecting consumers' private information cannot do it without 
the proper tools. Civil penalty authority is needed.
    Fourth, Congress should avoid the temptation to address 
complex challenges with a one-size-fits-all approach. There are 
different types of actors on the internet with different roles 
to play, different relationships with and commitments to 
consumers, different competition environments, and different 
abilities to solve problems. If we adopt a uniform regulatory 
approach to the entire internet, we are going to be left with 
the lowest common denominator, something like transparency with 
enforcement that just prohibits deceptive practices. That is 
not good enough. Consumers are asking for more.
    I appreciate your commitment to this issue. I thank you, 
and I look forward to your questions.
    [The prepared statement of Ms. Moy follows:]


 [GRAPHIC(S) NOT AVAILABLE IN TIFF FORMAT]
 
    
    Mr. Latta. And again, thank you for your testimony this 
morning.
    And Dr. Tucker, you are recognized for 5 minutes.

                 STATEMENT OF CATHERINE TUCKER

    Dr. Tucker. So, first of all, I would just like to say what 
a huge honor it is to be invited here today. Thank you very 
much for the invitation. What I want to do in my 5 minutes is, 
first of all, talk about some research I did into an apparent 
algorithmic bias and then talk about three implications for 
policy.
    Now, this particular research topic--what we did was we ran 
a field test on Facebook where we placed an ad which advertised 
job opportunities in science and technology. And we placed that 
ad, we also replicated it on Google and Twitter, and we found 
that the advertising algorithm ended up showing this ad for job 
opportunities in science to 40 percent more men than women. And 
on the face of it, this seems really quite concerning because 
obviously this is an area where we would like parity of gender 
opportunity.
    Now, I say on the face of it, it sounds concerning, because 
our research didn't stop there, which is usually how research 
stops, but instead we actually delved into the reasons why this 
apparent discrimination had happened. And we ruled out the 
usual leading explanations, which is either that humans are 
biased, absorb cultural prejudice, or the idea that somehow 
women have self-inflicted not seeing the ad on themselves by 
not reacting to it. Instead, if women ever saw the ad, they 
loved it. They clicked on it.
    Instead, what actually was going on is all in terms of 
understanding how the algorithm works, which is that an 
advertising algorithm basically runs an auction in real time 
where advertisers bid for eyeballs, and there were some 
advertisers out there that liked to show ads just to women, and 
as a result they pay more to show the ad to women. And because 
we had set up our ad to be gender-neutral, the algorithm 
thought it was doing us a favor by trying to minimize our costs 
and not show our ad to those expensive female eyeballs, but 
instead prioritize those cheaper male ones.
    Now, that takes us, you know, to show that actually 
economic forces actually shape a lot, you know, how we see 
algorithms work. And I want to just highlight three 
implications of policy. The first implication is that about 
algorithmic transparency. Now, algorithmic transparency just 
sounds wonderful, right? Who could ever argue with 
transparency?
    But, in this case, let's suppose we could ever decode the 
pages and pages of algorithms which underlie this ad auction. 
All we would find is an innocent algorithm trying to save 
advertisers money. It wouldn't give us really any insight into 
the potential for bias, and I think that is another argument to 
build on what we have heard earlier, why transparency, though 
just so beautifully sounding, is probably not a solution here.
    The second thing I want to emphasize is, it may be 
tempting, and we sort of, you know, we have heard a little bit 
of this idea that maybe the problem is not the algorithms, it 
is the data that feeds them. And I do want to caution the 
committee surrounding just simply restricting data flows in 
this economy. I have done some research. I have testified it 
into the past about the really quite hideous effects that 
attempts to regulate privacy in online advertising have had on 
the health and strength of the technology industry in Europe.
    We show that they had a 66 percent drop in efficiency after 
passing regulation, and you just have to sort of fast forward 
10 years, look at the strength of the American tech industry 
relative to Europe to see where that has led. I have also done 
some research in the U.S. We should emphasize that just 
restricting data in the health arena has actually led to some 
really quite negative consequences, such as hospitals failing 
to adopt potentially lifesaving neonatal technology saving 
babies.
    Now, the last--so that is why I am worried about 
restricting data as a solution--the last thing I just want to 
say is, look, in some sense you could write a headline saying 
``MIT professor finds ad algorithm doesn't show job ads to 
women,'' but imagine if I had found that for toothpaste. Would 
we be that worried? No, we might think, well, maybe men should 
see toothpaste ads, not that worried about it. So I do want to 
emphasize again the idea that it really matters, the outcome 
really matters. Thank you.
    [The prepared statement of Dr. Tucker follows:]

 [GRAPHIC(S) NOT AVAILABLE IN TIFF FORMAT]
 
    
    Mr. Latta. Thank you very much for your testimony.
    And Dr. Pasquale, you are recognized for 5 minutes.

                  STATEMENT OF FRANK PASQUALE

    Mr. Pasquale. Thank you very much, Chairmen Walden, 
Blackburn, and Latta and to Ranking Members Schakowsky and 
Doyle. It is a great honor to be here today.
    My testimony is based on my book, ``The Black Box 
Society,'' in which I distilled about 10 years of research into 
the role of data and algorithms and argued for the importance 
of transparency, and I am happy to do that today. I want to 
argue that the use of data and algorithms by large corporations 
will be at the core of civil rights, consumer protection, and 
competition policy for the 21st century. And I will go over 
each of those and then talk about how this committee can play a 
role in advancing all three of those goals.
    First, with respect to civil rights, I was very glad to 
hear from Congresswoman Clarke about the letter to Facebook 
with respect to discriminatory ad profiling. That was 
discovered last year by ProPublica. There were promises it 
would be addressed. It was not addressed. And I think that 
shows some of the failures of self-regulation in the area.
    Also in my testimony I talk about racial disparities with 
respect to ad delivery and disparities with respect to 
disability status or a health condition. For example, a credit 
card company deciding to raise the interest rate on someone 
once they know that the person went for marriage counseling. I 
think that is a very troubling sort of thing, and we should be 
able to look into that to get transparency about whether it is 
happening and to stop it.
    Secondly, with respect to consumer protection, Ariel 
Ezrachi and Maurice Stucke are great antitrust law scholars and 
they say that, given the information asymmetry between large 
corporations and consumers, consumers now really exist in a 
Truman Show. It is like a Truman Show online. They know so much 
about us, we often know so little about their practices, and 
they show how consumers can be manipulated by data that they 
don't know about.
    So, you know, we may hear a lot about good personalization 
online, you see things that you want, et cetera, but there is 
always a dark side to that. There are things, for example, like 
vulnerability-based marketing, where the marketing could be 
based on picking out people who are at particularly insecure 
times in their life or particularly insecure times of day for 
individuals. And I think this sort of vulnerability-based 
marketing, predatory loan targeting, all those things are 
troubling, and not just for traditionally protected groups but 
also for people, say, in rural areas that might be subject to 
price discrimination that I discuss in my testimony.
    I would also say that with respect to competition, the 
combination of the power of data in terms of enabling very 
large digital platforms to decide what consumers see, when they 
see it, what types of things that they are offered and not 
offered, that that leads to what I call a self-reinforcing data 
advantage. What I mean by that is to say that, if you are a 
large platform, you tend to have more data. When you have more 
data, you are able to target your things better to consumers. 
When you are better able to target to consumers, more consumers 
come on board.
    It is a virtuous cycle in a way, but on the other hand it 
does risk getting out of hand and creating the types of 
asymmetries that really you can't overcome as a competitor. And 
we have seen that, for example, with respect to European action 
against Google in their antitrust judgment against Google, 
where they talked about Google potentially privileging its own 
services over rivals in search results in ways that were opaque 
to consumers.
    And I think that we have got to look at those sorts of 
dynamics and start to address them. It will be hard, though. 
And, by the way, I would say that one reason maybe why the U.S. 
tech scene is doing better than the European one, you know, we 
have to look at these sort of competitive dynamics, as well, 
not just regulation. I would also talk about the black box 
effect here. I would say that it is very hard for us to know 
exactly what is going on, and we may have only seen the tip of 
the iceberg here. We may have only scratched the surface.
    Now, I have painted a very bleak picture of big data and 
algorithms in this testimony, but there is good news on the 
horizon. Over the past decade, a number of visionaries have 
developed a movement for accountability by users of algorithms. 
It took a combination of computational, legal, and social 
scientific skills to unearth each of the examples that I have 
discussed: troubling collection, bad or biased analysis, or 
discriminatory use of data. And I hope we talk about all three 
of those things today.
    Empiricists may be frustrated by the black box nature of 
algorithmic decision making, but they can work with legal 
scholars and activists if we have freedom of information laws 
and if we enable people to understand better how data is being 
collected, how it is being used, how it can lead to 
discrimination. Journalists also have been teaming up with 
computer programmers and social scientists to expose new 
privacy-violating technologies of data collection analysis and 
use, and they have pushed regulators to crack down on the worst 
offenders.
    I would conclude today by saying that U.S. lawmakers can 
really help by requiring the openness of algorithms used in 
many governmental contexts and moving on to empower people to 
have knowledge of what is going on and how their online lives 
are being ordered. With that, thank you very much.
    [The prepared statement of Mr. Pasquale follows:]

[GRAPHIC(S) NOT AVAILABLE IN TIFF FORMAT]

    
    Mr. Latta. Thank you for your testimony this morning.
    And Dr. Kearns, you are recognized for 5 minutes.

                  STATEMENT OF MICHAEL KEARNS

    Dr. Kearns. Thank you. Chairmen Blackburn and Latta, 
Ranking Members Doyle and Schakowsky, and other distinguished 
members of the subcommittees, thank you for the opportunity to 
testify at this important hearing. My name is Michael Kearns, 
and I am a computer and information science professor at the 
University of Pennsylvania. I am an active researcher in the 
field of machine learning, and I have consulted extensively on 
the use of machine learning in the technology and finance 
industries.
    The fields of machine learning and artificial intelligence 
now play a central role in virtually every sector in which 
large data sets are present. The number of instances in which 
the use of machine learning has provided tangible societal 
benefits, such as in medical diagnosis, is large and growing. 
Machine learning also increasingly plays a central role in the 
data collection and use practices of consumer-facing technology 
companies.
    Today I want to discuss data intimacy, which is the notion 
that machine learning enables companies to routinely draw 
predictions and inferences about users that go far deeper than 
the apparent face value of the data collected as part of online 
activities. It is not simply a question of whether consumer-
facing tech companies are collecting large volumes of data, 
such companies are collecting information that provides or 
allows inferences regarding intimate details about our personal 
lives.
    Search engine queries permit inferences about our physical, 
financial, and psychological conditions. Social media users 
routinely reveal intimate opinions, beliefs, or affiliations. 
For example, a recent study showed that using machine learning, 
anonymous social relationship data permitted accurate 
identification of romantic partners for over 55 percent of 
users. Another study concluded that Facebook's algorithms and 
models are capable of identifying social relationships of which 
its users are themselves unaware. And religious and political 
beliefs can be accurately predicted from apparently unrelated 
social search and shopping activity.
    Consumer-facing tech companies in the United States have 
amassed an almost unimaginable set of data about consumers, 
which enables machine learning and artificial intelligence to 
make predictions and inferences about consumer behavior and 
preferences. These large and diverse data sets are the 
foundation for effective algorithms and models, and companies 
compete vigorously to amass or acquire these data sets. For 
example, search engines provide vast amounts of data about 
consumers' interests in the manner in which they conduct 
searches. Similarly, mobile operating system data provides a 
treasure trove of information regarding virtually everything a 
consumer does on a mobile device as well as their physical 
location.
    In addition to knowing with whom a consumer affiliates 
directly, social media platforms are able to accumulate 
information about who a consumer follows or what he or she 
likes. However, while the quantity of data is critical to 
develop accurate algorithms and models, the quality and 
intimacy of such data is equally or more important in 
discerning consumer preferences and behaviors. Increasingly, 
machine-learning-based algorithms are utilized not only to 
determine consumer purchasing habits, but also to infer a 
consumer's emotions, moods, and mental states.
    While machine learning is employed most commonly and 
pervasively to target advertising as we have seen in the media 
recently, algorithms can also be utilized to generate or incite 
certain emotional responses. From a privacy perspective, 
perhaps the most important overarching conclusion is that the 
intimacy of consumer data cannot be measured by metrics that 
fail to account for the nature, diversity, and content of the 
data and, most importantly, its potential uses for modeling and 
inferences.
    It is both common and possible that the highest-volume data 
sources can reveal little about the consumers who generate that 
traffic, whereas more specialized data can directly and 
indirectly reveal the most private and personal details about 
consumers. In fact, the widespread application of machine 
learning to specialized consumer data sources is deliberately 
designed to extract personal and actionable insights about both 
individual users and collective behavior.
    It would thus be wrong to formulate privacy policy based 
only on the amount or apparent source of data. One must 
evaluate the sensitivity of the data as well as anticipate how 
private or intimate the inferences and predictions that could 
be made from the data might be. This challenge argues for a 
privacy framework that comprehensively covers the diverse range 
of data being used commercially and applies consistent 
technology-neutral privacy requirements.
    Thank you again for the opportunity to testify before you. 
Machine learning and AI present significant challenges for 
policymakers because of the rapidly evolving nature of the 
technology as well as its pervasive use among consumer-facing 
tech companies in predicting consumer preferences and drawing 
inferences about their lives. While policymakers should be 
mindful that machine learning and AI also produce many of the 
sizeable benefits inherent in consumers' online experiences, 
such technology enables companies also to both model and shape 
user behavior. Thank you.
    [The prepared statement of Dr. Kearns follows:]


[GRAPHIC(S) NOT AVAILABLE IN TIFF FORMAT]

    
    Mr. Latta. Thank you very much for your testimony. We 
really appreciate it. And this ends that portion of our hearing 
this morning. We will now be going to the questions from the 
Members, and I will begin the questioning and recognize myself 
for 5 minutes. And again I apologize for my 4 weeks of 
allergies, and I hope I get better in the next 4 weeks.
    Professor Kearns, if I could start with you. Algorithms are 
used to produce the results that we see on the internet such as 
when we do a search or see an advertisement. As policymakers, 
what are the key benefits and risks for consumers associated 
with these algorithms that we should be focused on as 
legislators?
    Dr. Kearns. Well, I think the benefits are, you know, 
pretty obvious to anyone who is a regular user of modern 
internet technology. The personalization in social media sites, 
in search engines, and in many other aspects and apps that we 
use, we all enjoy the benefits of that. I think to me, I think 
the greatest risks are the kinds of things I talked about, 
which is, you know, there is sort of a distinction about facts 
about you and things that can be inferred about you from those 
facts.
    And so it is one thing to, for instance, ask about 
disclosure or discuss what is actually, literally, in the data 
that is being collected, but that is kind of where the game is 
being played, as far as I am concerned. The use of machine 
learning allows one to make many inferences that are 
statistically quite accurate about consumers that aren't 
written down anywhere in the data about that consumer.
    So, you know, to give a personal example, the fact that I 
am an academic and, you know, use a Mac and drive a Subaru 
probably lets you guess my political affiliation quite 
accurately already, and if you knew a bunch of other facts 
about my online behavior, you could probably infer a great deal 
more. And there are many, many studies these days that sort of 
establish that fact, and this is a valuable thing to technology 
companies to be able to do that, to do this kind of--I think in 
one of the other testimonies here--this kind of 
microsegmentation.
    And I think this is the kind of thing that is hard for 
people to understand, and it is even hard for the scientists at 
these companies to understand the sort of power of this, this 
sort of predictive power that they have. You know, when these 
models are built they don't really know a priori and maybe even 
afterwards exactly what properties of consumers or inferences 
they are making about them that aren't--you know, they go well 
beyond the latent data itself.
    Mr. Latta. Thank you.
    Dr. Tucker, your research shows the tension between how 
much we say we value privacy and in reality how much data we 
are willing to share online to connect with friends or get 
personalized recommendations and coupons. What accounts for 
that disconnect, and how important is the context in what 
consumers are willing to share online?
    Dr. Tucker. Well, I am really thrilled to be able to talk a 
little bit about this because I didn't get to mention it in my 
testimony. And this is a so-called privacy paradox that so many 
people say they care about privacy but then act in ways which 
doesn't sort of live up to that.
    And one thing, we did a little study at MIT where we showed 
that undergraduates were willing to share really very personal 
data in exchange for a slice of cheese pizza. And that was even 
the ones--and what was slightly disconcerting about it was even 
the people who said that they really cared about privacy, they 
usually behave in accordance with those norms, but the moment 
they saw the cheese pizza was the moment they are willing to 
share the most personal information.
    Now I wish I could tell you that I found any group of 
consumers out there who were not--or any group of 
undergraduates who were not willing to share data for cheese 
pizza, but I didn't. So as of yet, answering your question is 
hard just because we do see this inconsistency between the way 
that consumers say they talk about their privacy and actually 
act out there in the online world.
    Mr. Latta. Thank you.
    Professor Ben-Shahar, your research indicates that 
consumers often view privacy policies as confusing and often 
ignore them, especially from your photograph. At the same time, 
mandated disclosure has been embraced in many laws and by many 
regulators. How should we balance the desire for transparency 
with the results of your research?
    Dr. Ben-Shahar. I think we should recognize that our desire 
for transparency, while well-intentioned and makes sense--very 
alluring, consistent with all American ideologies--all these 
transparency laws and mandated disclosure laws pass without 
opposition in this chambers or in any State chambers. This is 
the one unifying American law. I think we should also recognize 
that there is a good reason probably why it is so easy to enact 
these laws: There is nothing to them.
    And therefore I think that it is important to set them, 
cast them aside, and then that would enable us to actually get 
into the--I think in my book I give the example of medicine in 
the 19th century. Almost every disease was addressed by blood-
letting. It took the ability or, you know, from the medical 
profession to recognize that this is, you know, that panaceas 
don't work. You cannot use that to start figuring out solutions 
for each individual problem.
    And today you are talking, you know, I am invited to talk 
to you about data policy. I was invited by the FTC and before 
other agencies to talk about consumer lending, other contexts 
in which transparency and disclosure is the key regulatory 
technique, and I keep suggesting to them that it is in your 
area. You have to first ask yourself what the problem is.
    I think it is striking to hear what Dr. Tucker and others 
are finding, that statements about the magnitude of the 
problems are not matched by the behavior and economic reality. 
Data privacy is a nice kind of buzzword and data security we 
are really worried about, we can brandish the number of people 
that were hurt by the different--were implicated by the 
different breaches that occurred, security breaches.
    But what is the evidence about actual consumer harm? Most 
of the lawsuits that followed, you know, the lawsuits that have 
followed the Target breach and the Equifax breach were by 
merchants, credit card companies, banks, they are suffering a 
lot of the--because our laws largely protect consumers from 
these incidents. So I think I do not want to suggest that there 
is no harm in these areas, but it is critically important to 
understand its magnitude before we begin to think about 
solutions.
    Mr. Latta. Thank you very much. And, since I ran over, I 
will recognize the gentlelady from Illinois, the ranking member 
of the subcommittee, and also give you a little more time on 
your questions.
    Ms. Schakowsky. Thank you. You know, it is hard to decide 
who to really focus on because we only have 5 minutes. You 
know, when it comes to transparency, not only don't I take the 
time to read it, but in order to get to my goal if I don't hit 
Accept, I Agree, then I can't finish the transaction. So most 
of the time, for both reasons, I just accept and move on.
    But I do want to talk about enforcement, and therefore I 
want to ask Ms. Moy some questions. In Chairman Blackburn's 
opening statement she talked about shifting privacy from the 
FCC, the Federal Communications Commission, to the Federal 
Trade Commission, so I think it is important to understand how 
the FCC and FTC differ, you alluded to that. But so, Ms. Moy, 
can you briefly describe the FTC's authority, if any, to issue 
regulations?
    Ms. Moy. The FCC or--I am sorry, the FTC really doesn't 
have authority to issue regulations. It can issue rules under--
it can issue Mag-Moss rules, but it is extremely difficult to 
do that, and as a practical matter nearly impossible. It can 
issue rules under the Children's Online Privacy Protection Act 
and has done that rather effectively, and the Safeguards Rule 
under GLBA.
    But when it comes to general privacy and data security 
obligations, the FTC is unable to issue regulations.
    Ms. Schakowsky. So the FTC can't use the typical notice and 
comment rulemaking process to issue regulations about what 
personal information platforms can collect from users or how 
those platforms can use that personal information to determine 
what content it shows to users, correct?
    Ms. Moy. That is right.
    Ms. Schakowsky. So which means the Commission is limited to 
bringing enforcement actions after unfair, deceptive practices 
have been committed, and often after consumers have been harmed 
already, right?
    Ms. Moy. Yes.
    Ms. Schakowsky. So let's talk about the FTC enforcement 
tools. In your written testimony you wrote that, quote, ``the 
FTC generally can only take enforcement action against entities 
that use consumer information in ways that violate their own 
consumer-facing commitments.'' Can you describe what do you 
mean exactly by consumer-facing commitments, and are you 
referring to policies like the terms of services and privacy 
policies?
    Ms. Moy. That is right. The bulk of the FTC's privacy and 
data security authority comes from Section 5 of the Federal 
Trade Commission Act which authorizes it to prohibit unfair and 
deceptive trade practices. As a practical matter, the FTC 
almost never enforces unless it determines that there is 
deception that has occurred, and it evaluates a possible 
deception based on something that a company has said perhaps in 
a privacy policy and then trying to figure out whether or not 
it has violated that.
    Ms. Schakowsky. Even when a platform does violate its own 
policies, the FTC's remedies are limited. As you noted in your 
written testimony, the FTC cannot impose a fine against that 
platform. What are the remedies available to the FTC?
    Ms. Moy. Exactly. Yes, you know, and as I mention in my 
comments, I think the authority of an agency is only as good as 
its enforcement is. And when it comes to the FTC, although it 
can bring actions for deception when as it relates to privacy 
and data security, with few exceptions it cannot levy civil 
penalties against companies that violate privacy and data 
security commitments. And as a result there is very little in 
way of teeth when it comes to the FTC's authority.
    Ms. Schakowsky. So I know that both Acting Chairman 
Ohlhausen and Commissioner McSweeny support giving the FTC 
civil penalties authority, and I believe you do, too, as well. 
Is that right?
    Ms. Moy. That is right.
    Ms. Schakowsky. And do you think it would benefit consumers 
if the FTC had authority then to issue regulations under the 
normal notice and comment process?
    Ms. Moy. I do. I think that the fact that the vast majority 
of consumers are asking for greater consumer privacy protection 
and for the law to be stronger in this area suggests that we 
would benefit greatly from greater authority for the FTC or 
another agency.
    Ms. Schakowsky. Well, so are there other things that 
Congress can do? I mean, you alluded maybe to other agencies to 
help strengthen the FTC's ability or some other agency to 
protect consumers.
    Ms. Moy. Well, in addition, as of right now the Federal 
Trade Commission can't actually regulate the actions of common 
carriers, and that is a major problem that, particularly with a 
recent case or a case that is currently pending in the Ninth 
Circuit, it is unclear whether the FTC has any authority at all 
to enforce the privacy and data security obligations and 
activities of companies that have any common carrier practice 
at all. So internet service providers that offer--whether 
broadband is classified under Title II or not, the FTC may well 
not be able to.
    Ms. Schakowsky. So in the short term what should we be 
considering?
    Ms. Moy. In the short term I think that we do need strong 
protection, privacy by default, ideally, for entities where 
consumers have no choice but to share information. And I also 
think that we need to preserve existing protections. We need to 
preserve existing protections at State law as well as existing 
protections under regulations like net neutrality.
    Ms. Schakowsky. Thank you. I yield back.
    Mr. Latta. Thank you. The gentlelady yields back.
    The Chair now recognizes the chairman of the Communications 
and Technology Subcommittee for 5 minutes.
    Mrs. Blackburn. Thank you, Mr. Chairman, and thank you all 
for your testimony. It is so enlightening, and we appreciate 
it.
    And Dr. Kearns, I am going to come to you first. Thanks for 
the work you are doing on privacy and around those elements, 
and we have had a lot of focus on privacy here. And earlier 
this year I had introduced the BROWSER Act, and basically it 
has two guiding principles, things that many of us think are 
very important. One is that we have to find a better balance on 
privacy moving toward giving the consumer more information and 
more control over, as I term it, their virtual you, their 
information that is being collected and used and sometimes 
distributed; and then, secondly, that consumers have the very 
same privacy expectations across the entire ecosystem. They are 
not distinguishing between the ISPs and the edge providers, so 
when we are setting the ground rules on privacy, they should 
reflect that.
    So I would like to hear what your thoughts are on those two 
points. And when we are talking about online privacy, do you 
think that people make that distinction? When we are talking 
about appropriate balance, where is that appropriate balance 
within opt-in where the consumer owns that information or 
either opt-out? So I would love for you to talk about that for 
a minute.
    Dr. Kearns. Yes. These are good questions, hard questions. 
First of all, to preface, I don't have specific policy 
recommendations on these issues. But as a scientist, when I 
think about the landscape for consumer privacy, the first thing 
I think about is kind of how actionable the data being 
collected is and sort of at what level of abstraction it is. 
And furthermore, there is a phrase I like to use, which is 
``data triangulation,'' which refers to the incredible power 
you can get from having multiple sources of data about the same 
individual.
    So to me, you know, when I think about privacy, the things 
I worry most about are cases in which there are parties that 
are collecting sort of very private, intimate data on the one 
hand and also many different sources of it. So, to give an 
example, you know, by seeing what you buy I can know a lot 
about you. By seeing, you know, what you search for I can know 
much more about you. By knowing not only those things but where 
you are, that gives me a great deal of more information. And if 
you, for instance, let me also maintain your calendar for you, 
then I also know where you will be in the future.
    And I think that the, you know, greatest privacy concerns I 
have are at that level, at the level where people are very 
directly expressing, you know, things that might be quite 
private, things that they wouldn't express in public forums, or 
that they are expressing in a public forum like a social 
networking service but are completely unaware how strong the 
correlations are between their own behaviors and their friends' 
behaviors and their other online behavior.
    And so I think in terms of helping consumers understand the 
privacy landscape it is important not to ignore any source of 
data. I am not claiming that ISPs, for instance, aren't also 
collecting very large amounts of data, but to me, I personally 
am much more concerned about the data I kind of willingly give 
away using a search engine, and then also letting my operating 
system track my location online or the presence of beacons in 
retail stores that kind of correlate my online and my offline 
behavior.
    Mrs. Blackburn. Great.
    Ms. Klonick, I want to come to you on economic incentives 
and the economic incentives that the platforms have to use 
algorithms to curate selective content. And I think Dr. Kearns 
used the term ``microsegmentation'' as they are looking at that 
for users, you know, based on this online activity. Would you 
agree that the platforms are being paid to prioritize certain 
content over other content? And touch on the free speech 
implications there.
    Ms. Klonick. Yes. Insomuch as advertised content is paid 
content over their user content, I think that these, you can 
absolutely prioritize certain types of content. I am not 
familiar with the algorithmic processes that would prioritize 
one user's content over the content of another and that they 
are being paid to do so right now, but the free speech 
implications of the vast power of these platforms to self-
regulate is, they are twofold.
    One, it has a lot of implications for the user's speech 
rights in how these private platforms can unilaterally control 
at what goes up and what stays or goes down on their sites. But 
also these platforms have free speech rights, arguably, free 
speech rights, themselves. So their right to create the 
community at Facebook or at Twitter, for example, is arguably 
their own First Amendment right.
    Mrs. Blackburn. Dr. Tucker, on the economic incentives, do 
you think that some of these platforms should be willing to pay 
consumers or users more than a free slice of cheese pizza?
    Dr. Tucker. Wonderful question. Now, this is a very 
interesting question. So the slice-of-cheese-pizza example was 
really about the consistency between what people say about 
their privacy and then how they act.
    Now, in terms of paying for data, there have been many 
experiments, some of them launched from Cambridge, 
Massachusetts, where various startups have helped devise, have 
tried to actually set up markets for data. And the reason that 
is so attractive is, from an economics point of view, one way 
of thinking about privacy is, really, there is a lack of 
clarity about property rights. So a market for data is an 
attractive notion.
    Now, in all of the instances, though I have been really 
excited at the beginning because of the idea of actually 
setting up a market for data and paying consumers, all of these 
platforms have failed for the simple fact that the kind of 
consumers they attract who want to exchange their data in these 
markets tend to be, how can I say it, the less commercially 
exciting consumers. And we have had this problem of actually 
just setting it up, making these markets work just because we 
haven't been able to get the right set of consumers.
    So I think it is a wonderful idea. I hope one day we will 
get it to work. We haven't yet.
    Mrs. Blackburn. Yield back.
    Mr. Latta. Thank you very much. The Chair recognizes the 
gentleman from Pennsylvania, the ranking member on C&T, and I 
also yield you the long time, too.
    Mr. Doyle. Well, Mr. Chairman, this terrible precedent that 
you have started by allowing everybody to go 2 minutes over, I 
am going to try to get us back on track and just use my 5 
minutes.
    You know, when you think about all this technology--social 
media, the internet, artificial intelligence--you know, the 
most wonderful, horrible invention in the world, and as 
consumers we tend to look at the bright side of all this 
technology without understanding the dark side. If anybody 
thinks they have privacy, the only way you have privacy today 
is, I call it to go Flintstone, to have the old flip phone, to 
not be on Facebook or Twitter or any of these social media 
sites.
    But, you know, the reality is, for most Americans over 80 
percent of the land mass of country, most Americans have only 
one ISP provider. They don't even have choice when it comes to 
that. And so they go on their ISP, and it is the only one they 
have, and they tell you how they are going to use your data, 
and it is about 20 pages long of a bunch of legal jargon that 
most attorneys probably couldn't understand. And if you don't 
click I Agree, that is it, you don't have access to any of 
this.
    So you don't even need a cheese pizza to get people to give 
up their information. They want to go online to do whatever it 
is they want to do online, and the only way they can get there, 
especially if they only have one ISP, is to do that. Now, 
search engines, you have some choice and you can read 
different, you know, policies on search engines of how they use 
your data, and it varies online, whether you are on Google or 
whether you are on DuckDuckGo or these various sites, at least 
you have some choice. With your ISP, most Americans don't have 
choice. They have one place to go.
    And it is kind of ironic that we are here today to discuss 
concerns about algorithms used by these social media companies 
to curate content on the internet, but as we speak, over at the 
FCC the Chairman is getting ready to allow broadband providers 
to block and edit speech on the internet at their discretion, 
relying on public commitment from these providers that they are 
going to behave.
    And, given the Ninth Circuit case casting doubt on whether 
the FTC may even police these broadband companies, it is sort 
of creating a situation where broadband companies are just free 
to reign over consumers with impunity, and the FTC for all 
intents and purposes is a toothless tiger. We talk about 
shifting all this watchdog function over to the FCC----
    Ms. Schakowsky. FTC.
    Mr. Doyle [continuing]. And they don't really have the 
ability to do anything on behalf of consumers. Right now, if 
this law passes on net neutrality next month, basically there 
is no law of the land, we are just trusting people to behave. 
They are saying they are going to behave, and we are going to 
take them at their word that they are going to behave.
    Professor Moy, I wonder if you can give us some examples of 
how broadband providers behaved prior to the enactment of 
strong bright line rules that were put in place by the FCC in 
2015?
    Ms. Moy. Thank you, Representative. That is a great 
question. Right, because before we had rules we did see 
broadband providers, internet service providers, blocking 
things like Voice over IP, blocking tethering applications, so 
they could extract more from consumers in monthly fees, 
blocking peer-to-peer sharing applications. AT&T threatened, I 
think, to block FaceTime unless consumers agreed to pay more 
for the ability to use that.
    So, you know, we certainly have seen examples in the past 
of ISPs using their power as gatekeepers to prevent consumers 
from using services that may well want to----
    Mr. Doyle. So tell me what recourse would consumers have if 
the FCC Chairman gets his way and removes these protections?
    Ms. Moy. It is hard to see how they would have any recourse 
at all. I mean the FCC plans to rely on the consumer-facing 
commitments again of ISPs, but it is unclear whether ISPs would 
actually be required to commit to not prioritizing content, not 
blocking content. And even if they did make those commitments 
and then violated them, the FTC--you know, you mentioned the 
Ninth Circuit case--may not be able to enforce against them. 
You know, their enforcement authority against ISPs is going to 
be questionable at best or nonexistent at worse. And even if 
they could enforce, again, they don't have civil penalty 
authority.
    Mr. Doyle. Thank you.
    Mr. Chairman, in the spirit of staying within 5 minutes, I 
have 5 seconds left, and I will yield them back to you.
    Mr. Latta. Thank you very much. The gentleman yields back. 
And at this time the Chair recognizes the gentleman of the full 
committee for 5 minutes. The chairman, the chairman of the full 
committee.
    Mr. Walden. OK, thank you all, I appreciate it. And thanks 
for our witnesses. My apologies for having to come and go a bit 
today, but we do appreciate your written testimony and the 
answers to our committees' questions.
    I guess, Dr. Moy, the question I have because we are 
concerned about misbehavior by ISPs, I am also concerned about 
misbehavior by others in the ecosystem of the internet. And it 
strikes me that on these information platforms we have seen 
foreign actors try to affect our elections with paid 
advertisement that is targeted.
    We know that there is, in effect, paid prioritization on 
some of these platforms, right, because you buy advertising, 
and it strikes me that at least Google--it is an amazing 
American company, it does incredible work but has about 77 
percent market share of search, and I have had consumers 
complain to me about what they believe to be the use of 
algorithms that have disproportionately affected them.
    So what--and maybe this can go to everybody on the panel, 
but so if, who governs the edge providers when there are 
questions about use of private data or--nothing is private 
anymore, but your data and how that gets--and I don't mean this 
in a negative way, but manipulated use through the algorithms, 
which we are all trying to get a better handle on, so who 
governs their activities and what enforcement protocols are in 
place for those?
    And I will start with you, Ms. Moy.
    Ms. Moy. Great. Thank you so much for the question. So yes, 
right now those practices are, in theory, governed or regulated 
by the Federal Trade Commission, enforced by the Federal Trade 
Commission, again under this idea that they can enforce 
consumer-facing commitments. But, you know, I think you raise a 
really good point, which is that the growing power of these 
platforms to editorialize on content is potentially 
problematic, and we should explore possible solutions to that.
    But in the meantime, the last thing that we should be doing 
is eliminating protections that consumers have against paid 
prioritization at the network level, where there is very little 
transparency.
    Mr. Walden. Right. But in terms of other enforcement in the 
overall ecosystem, if I have a complaint against a search 
engine or I have a complaint against my social media, I go to 
the--my only recourse is the Federal Trade Commission, which 
you have said doesn't have the kind of enforcement authority 
you would like to see it have, correct?
    Ms. Moy. Right, right. Yes, indeed. And, you know, and 
staff and Commissioners----
    Mr. Walden. Do you think there should be greater authority 
for enforcement over the edge providers or similar to what you 
would see over the ISPs?
    Ms. Moy. I would certainly support adding protections for 
consumers across the board. I think that there are concerning 
practices by both types of actors. I would caution this 
committee against exploring a one-size-fits-all solution to----
    Mr. Walden. Why?
    Ms. Moy. Because I think that, you know, again the types of 
information that various actors have access to is different. 
The commitments and relationships with consumers that they have 
is different. For example, consumers are paying dearly for 
monthly access to the internet with a broadband provider, 
whereas they often are getting certain other services for free 
or----
    Mr. Walden. Right. No, it is an exchange of value. Yes.
    Ms. Moy. There are certainly differences between different 
types of actors as well the availability or lack thereof of 
sharing information with a particular provider or particular 
type of actor.
    Mr. Walden. So let me ask you a question, because we have 
also heard before this committee that there is a very high rate 
of encrypted data that passes through the ISP pipes, if you 
will allow me to use that term, and that that is encrypted. 
They don't know what those data are. It is encrypted, it goes 
through. It is well over 50 percent, perhaps, so they don't see 
it, but the other platforms do see the data and can use it and 
do use it in that exchange, as we know. I am not saying these 
are bad things.
    And I think we have heard--I believe it is Dr. Tucker. I am 
going to get them to make those nameplates bigger for us old 
people that have vision issues. But the point is that they can, 
they see it differently. Can you address that, the differences 
you have seen in Europe versus here maybe on how our technology 
has expanded dramatically and innovation here because we 
haven't cranked down as much, right, on privacy?
    Dr. Tucker. OK. So in the past--and this was about 2011--I 
did research on how some of the early European data privacy 
regulation really stymied the ability of Europe's ability to 
create additional ecosystem like we have now. And since then 
there has actually been follow-up research which has shown that 
it wasn't just at the beginning, but it has kept on going, and 
we have seen an awful lot of lack of entrepreneurship in 
Europe, too.
    And so we have seen the failure at the beginning and then 
the follow-on failure of entrepreneurship, and I think to me 
that is what has really distinguished what we have seen in the 
U.S. tech sector.
    Mr. Walden. So we have had, am I accurate to say we have 
had more of a light touch regulatory approach to the internet 
up through 2015 from Europe?
    Dr. Tucker. I think it is certainly true that we have had a 
sector-specific touch, right. That we have focused on areas we 
might care about such as health, private financial data, 
children, rather than going for a broad brush approach.
    Mr. Walden. All right. I have exceeded my time. Thank you 
all again for your testimony, it is very helpful in our 
discussions, and I yield back.
    Mr. Latta. Thank you very much. The chairman yields back 
his time. And at this time the gentlelady from California, Ms. 
Matsui, is recognized for 5 minutes.
    Ms. Matsui. Thank you very much, Mr. Chairman. I want to 
thank the witnesses for being here with us today.
    I have a question, I think, for Ms. Moy right now. In 2015, 
the Office of Management and Budget issued a memorandum 
requiring all publicly accessible Federal websites to only 
provide service through an HTTPS connection by the end of 2016, 
which was last year. HTTPS protocol ensures that a consumer's 
connection is encrypted from their devices all the way to the 
Federal Government's systems. Regular HTTP connections sent in 
plain text can be intercepted and exploited by anybody or 
anything between the user and the website, including somebody 
using public Wi-Fi. A study released earlier this month 
revealed that only around 70 percent of Federal websites 
employed HTTPS protocol.
    Ms. Moy, how important are the security standards like 
HTTPS to protect the confidentiality of internet-delivered data 
on both Federal and commercial websites?
    Ms. Moy. HTTPS is very important. HTTPS would encrypt in 
transit the information that is transmitted via websites. So, 
for example, if you fill out a web form, for example, perhaps 
in an application for a service that you might find on a 
Government website, and that form contains or asks questions 
about information that is highly private, such as information 
about financial status or personally identifying 
characteristics like Social Security number, then, if the site 
is not employing HTTPS technology, one could mount an attack on 
the transmission and potentially read the information that was 
transmitted.
    Ms. Matsui. So how would you know whether it employs the 
HTTPS on the Federal website?
    Ms. Moy. So this is the type of thing where in a browser 
bar, you know, you will see up at the top the little, now we 
have that little icon, the little green lock that indicates 
trust for HTTPS protocol.
    Ms. Matsui. OK, something what we never look for, anyway. 
OK, thank you.
    I want to talk about embedded networks. Across almost every 
industry, we are seeing a trend towards embedding 
communications functions into their structures. Applied data 
science such as a massive internet of medical things, rely on 
faster, more efficient, and more robust communications with 
innovative enabling technologies such as blockchain. Blockchain 
can facilitate the exchange of massive amounts of data, but as 
a decentralized ledger technology it can make online 
transactions faster and cheaper while maintaining and 
protecting data integrity.
    Anyone on the panel, how can new digital technologies and 
applications help consumers improve data security? Anyone want 
to start on that one?
    Dr. Tucker. Well, I have written a little bit on 
blockchain, so I am just so excited that you mentioned it, and 
I am glad that you mentioned it without mentioning bitcoin, 
which is always a distraction.
    Ms. Matsui. It is a distraction.
    Dr. Tucker. And certainly we have got an initiative at MIT 
which gives enormous optimism for the kind of process that you 
are describing where, really, what we call verification costs 
for making these kind of transactions easier.
    Do I have any caveats? My only caveats are that when we 
have studied it, and if we are thinking about blockchain as 
being a recipe for protecting privacy, that in some sense it 
can sometimes embolden people to be somewhat more careless 
about their data surrounding the edge providers who are trying 
to serve the blockchain. And so, for example, we have seen that 
the mere mention of blockchain encourages people to share 
really quite personal information such as telephone numbers and 
so on without any guarantees of protection.
    Ms. Matsui. So they feel like it is much more safe because 
of the blockchain. They just figure that what they have heard 
about it, that this is a safe way to go?
    Dr. Tucker. Yes. That is right. So I sort of have the 
analogy that it is a bit like, once you have your seatbelt on, 
perhaps you drive a bit too fast, that kind of an analogy. And 
so I think it is definitely a step forward, but we have to 
realize that of course it is going to interact with other 
providers, and the most will be privacy concerns there.
    Ms. Matsui. Thank you.
    Did you want to make a comment?
    Mr. Pasquale. I would just say very briefly that I 
testified in September before the Senate Banking Committee, and 
I mentioned in part of my testimony futurist financial 
technologies such as blockchain. And I think that it is just 
very important to distinguish between the private permission 
blockchain and the public permissionless. I have a lot more 
confidence in the sort of private permission because it 
involves what I call complementary automation technology 
complementing individuals rather than replacing them.
    So I think that it is, just in terms where I have hope, it 
is more in that latter category of private permission 
blockchain.
    Ms. Matsui. OK, thank you. And I see my time is expired. I 
yield back.
    Mrs. Blackburn [presiding]. The gentlelady yields back. Ms. 
Matsui, I just mentioned to counsel that we may want to secure 
his Senate testimony and submit that into the record in 
coordination with your question.
    Ms. Matsui. Thank you very much.
    Mrs. Blackburn. Agreement? So ordered.
    [The information appears at the conclusion of the hearing.]
    Mrs. Blackburn. Mr. Shimkus, 5 minutes.
    Mr. Shimkus. Thank you, Madam Chairman. It is great to be 
here. I got to listen to your opening statements. I found them 
all very interesting. And then I had to run upstairs to do 
Energy Markets and Interconnectivity, and then I came back down 
here, so I may have missed a few issues.
    I just want to put on the record on this whole net 
neutrality debate, it is just, for a lot of us it is what is 
the enshrined law by the legislative process versus what a 
regulator decides what to do. And what we are seeing now with 
the passing of the Obama administration, and the Trump 
administration, is I kind of explain to my constituents it is a 
pendulum. We are going to do it this way, now we are going to 
do it this way, now we are going to do it this way, and to stop 
the pendulum you have to pass a law. You have to enshrine that 
into a statute, and I would encourage my colleagues to come 
together to do that.
    I also want to incentivize build-out. I like more pipes 
versus less pipes, and I don't want the Government deciding how 
one pipe should be structured. I would rather have so many 
pipes that everybody gets what they want when they want it at 
the speed that want it, and if you are a market-based 
conservative you have got to send a price signal.
    And then the other issue on that is this whole--part of 
this was kind of paid prioritization, or we are talking about 
so small of lag of time that I can't even use the proper 
terminology. But would I rather have lifesaving telemedicine go 
fast versus a Three Stooges video? The answer is yes, I would. 
So I just want to put that in the guise of some of the debates 
based upon what the FCC is considering. And then I want to 
segue real quick to this whole--this is a fascinating panel 
because you all have done, brought pretty much a different 
focus and sometimes there are similarities on privacy, on 
algorithms, on data.
    So I want to use this example. Over the Thanksgiving break 
I visited Washington University, a major medical facility in 
St. Louis, and so I briefly drew my little DNA strand, right, 
here. And so the question with data is in the healthcare arena 
we want to go to drive to personalized data, I mean 
personalized medicine, and personalized medicine means we 
understand the DNA sequence, and we can pull that out. So then 
a cancer patient, we don't have to try 15 different types of 
cures, we can direct it.
    Now that creates a lot of issues public policy-wise. One 
issue is the data collection. The other one is data sharing. 
The other issue is privacy. And when you are doing medical 
research, I mean, you are really trying to share that data, 
that DNA sequence of this one case across different major 
schools of medicine across the country and probably across the 
globe.
    So that goes to a lot of your individual comments. I kind 
of want this to happen. I really believe in personalized 
medicine. I think it is going to be a huge savings, and I think 
it helps treat the patient quicker and return them to a very, 
you know, return life. And we have these hurdles that we are 
all discussing here.
    Anyone want to weigh in on--Mr. Pasquale, and then I will 
go to Dr. Tucker. I got about a minute, 2 minutes left.
    Mr. Pasquale. I will be very quick to say that I completely 
agree with you, and I think that, you know, we have talked to--
I run a health law podcast with Nick Terry called ``The Week in 
Health Law,'' and we talked to several people who are law and 
policy experts in this type of area, sensitive health data, and 
we get a lot of good advice on, you know, how can we develop 
best practices in order to enable data liquidity, data flow 
between institutions.
    But I would also say, you know, based on some of the great 
work done by Sharona Hoffman in her article ``Big Bad Data,'' 
that sometimes if we don't have good data practices so we know 
where data comes from and where it is going to, that may impede 
the scientific validity of some of the findings. So I think we 
have heard a lot about privacy impeding innovation, but there 
are ways in which good data practices, good record keeping, can 
actually help promote innovation as well and promote scientific 
validity.
    Mr. Shimkus. Thank you.
    Dr. Tucker?
    Dr. Tucker. So I have a study coming out, it is forthcoming 
at Management and Science, where we actually look at different 
types of regulation and how they promote or don't promote the 
kind of personalized medicine you are talking about. And what 
we found there was that basically just focusing on consent was 
really quite harmful to patients being willing to adopt this 
kind of or sort of give this kind of unique data in a cancer 
treatment setting.
    What did seem to work, though, was actually giving control 
to patients, and there were some States that actually 
experimented with creating ownership or property rights over 
genetic data, and we have actually seen quite a bit of efficacy 
in terms of promoting personalized cancer treatments in those 
States.
    Mr. Shimkus. Anyone else want to weigh in? I really 
enjoyed--again I am having a hard time, too, with Mr. Ben-
Shahar on the statements of--I mean, how many of us get 
financial booklets after the fiscal year, and how many people 
throw it away? I bet you 99.99 percent of all people who get 
those booklets on what you should know. And I think it is a 
protection. It is really a protection for those people who are 
controlling our data. ``OK, we have done it. We have given you 
the information, now it is your fault if you don't follow it.''
    So, it is a great hearing. I appreciate everybody being 
involved. And I yield back my time.
    Mrs. Blackburn. The gentleman yields back and, Mr. Green, 5 
minutes.
    Mr. Green. Thank you, Madam Chairman. And I want to thank 
our two chairs and two ranking members for the hearing today, 
and as well as our witnesses.
    It is pointed out that personalized content that we all see 
on various online platforms is curated by both humans and 
algorithmic technology. However, the potential for harm from 
algorithms can be particularly difficult for Congress to 
address, and thus we should be focusing on it.
    Professor Kearns, in your testimony you point out that 
machine-learning-based algorithms can be used to determine a 
consumer's emotions at any given point in time. How do you 
monetize that?
    Dr. Kearns. Well, the short answer is I don't know. But 
certainly, if I can shape people's moods and it seems plausible 
people might be more willing to shop if they are in a good mood 
rather than a bad mood, that might be one way that I could 
monetize it. I think more generally, though, knowing detailed, 
fine-grained information about people's mental and emotional 
states in addition to, for instance, knowing about medical 
facts about them and their fitness level and their financial 
health, et cetera, I mean, it has clear sources of 
monetization.
    And some of my colleagues on the panel have mentioned some 
of the negative ones already, such as targeting groups that are 
particularly vulnerable at a particular time. There is a great 
deal of documentation, for instance, on kind of predatory loan 
practices online in the arena of for-profit education, for 
example.
    Mr. Green. OK, thank you.
    Professor Pasquale, if a person often does online searches 
for phrases that might signify challenging financial 
circumstances such as financial counseling, how might that 
change the ads and the search results that they see online?
    Mr. Pasquale. Oh, that is a terrific question. And one of 
the big worries that a lot of advocates have is that we can 
route people into different opportunities. So, for example, if 
you have exactly the type of searches that you are mentioning, 
someone might be routed towards payday loans, others might be 
routed away from them. Now to Google's great credit, I think, 1 
or 2 years ago, working actually with Georgetown, they started 
some self-regulation where they said, ``We are not going to 
have certain ads on that are over 36 percent APR.'' And I think 
that is very important, but I also worry that, you know, kind 
of competition concerns might arise if, for example, Google 
owned its own finance company that had a business model that 
would be advantaged by that particular rule.
    So I think we have to balance, you know, we have to both 
encourage tech giants to try to self-regulate to avoid the type 
of tracking that you are invoking, but we also have to have 
outside authorities to be able to watch that self-regulation, 
as well.
    Mr. Green. Or just so the consumer knows that, you know, 
that is being done and you might not be getting some other 
offers, that somebody else is making that decision on what they 
are presenting to you.
    Another question I have, you mentioned in your testimony 
that in 2016 after Facebook was found to be enabling 
discriminatory housing ads, it promised to change the system to 
address that issue but has not done so. Could you talk about 
efforts that Facebook and who might require Facebook to fix 
this problem, and why they may not be successful?
    Mr. Pasquale. Yes. I think that the issues here are, it is 
a complex ad ecosystem and so there are lots of different 
moving parts in the ads, but I think that what is disappointing 
is that there was this expose in ProPublica, there was a lot of 
attention to it. There were pledges to do better, but we just 
saw in the past week or so that the same people that exposed 
the original problem, that they are saying it hasn't been 
solved.
    So I think that is, again, another example where we have to 
empower either State or Federal regulators to actually have 
some teeth and to impose some of the penalties that would 
actually lead to a positive response.
    Mr. Green. As I found out in this job, everybody needs the 
boss and has to answer to someone. So we don't have an agency 
that can do that right now with Facebook if they agree to do 
something and do not do it?
    Mr. Pasquale. I think that there are possibilities with 
respect to, say, the deceptiveness or unfairness authority at 
FTC. I would also have to research with respect to the 
Department of Housing and Urban Development and its own 
enforcement practices, but that is not something that I have 
personally looked into, so I would have to look into that. Yes. 
And I could send that later on to the committee, yes.
    Mr. Green. Professor Kearns, you advocate for a policy 
approach to the extraction of consumer data that is 
technologically neutral and accounts for the sensitivity of the 
data collected. My question, can you elaborate on what you 
think that policy might look like?
    Dr. Kearns. Yes. I mean, first of all, maybe let me take 
the opportunity to say one thing that I think has been running 
through my head but I haven't been able to get out yet, which 
is especially on issues of discriminatory behavior by 
algorithms, I do think that there are scientific things that 
can be done to address this and there is a, you know, not small 
and growing community of AI and machine-learning researchers 
who are trying to design algorithms explicitly that meet the 
various fairness promises and guarantees.
    And it is still very early days, but this sort of idea of 
endogenizing some kind of social norm like fairness or privacy 
inside of an algorithm I think is extremely important, because 
while regulatory and watchdog agencies will always be very 
important, you know, the way a computer scientist would put it 
is they don't scale, right. So, if instances of malfeasance or 
privacy or fairness violations have to be caught by human 
organizations looking at, you know, specific instances or 
behaviors, they just won't keep up, right, because the tech 
companies are doing this at massive scale in an automated way.
    In terms of what can be done, you know, I think it is 
possible to audit algorithms for various kinds of behaviors 
without compromising the proprietary nature of the models or 
algorithms used. And a rough analogy I would offer are kind of 
the stress tests that banks have been subjected to on Wall 
Street where, you know, you have to demonstrate certain 
properties of behavior of your algorithm, but you are not, you 
know, releasing the source code for it.
    And I, you know, without having super-specific suggestions 
in that regard, I think that that is a promising general 
direction for policy and one that can balance between, you 
know, a company's legitimate right to preserve their 
intellectual property and consumer and societal concerns about 
the behavior of those algorithms.
    Mr. Green. Thank you.
    Mr. Chairman, I know I am over time. I appreciate your 
courtesies.
    Mr. Lance [presiding]. Thank you very much, Mr. Green, and 
I recognize myself.
    Ms. Klonick, in your testimony you mentioned choice as a 
key part of regulators' decisions not to pursue Title II-like 
regulations for online platforms. Title II-style regulations 
may be inappropriate for edge providers or for others in the 
internet ecosystem, as well. However, some have argued there 
are fewer choices among online platforms because each website 
or application serves a specific audience with a specific 
service. Would you please comment on that? Thank you.
    Ms. Klonick. Yes. I agree with that statement generally, 
that specific platforms speak to a specific audience. But there 
is an enormous and incredibly important distinction to be made 
here, and that is that there is a huge difference between 
companies that have kind of natural monopolies like ISPs and 
then content platforms like Facebook and Twitter.
    And the former kind of a piece of the pipe, or to put it in 
terms of speech, they are kind of the printing press and you 
don't want the printing press rearranging letters or blocking 
out sentences. You want it to be content-neutral to a certain 
extent, but you do want the paper or the writers or the editors 
who use that printing press to be able to make decisions based 
on the content, and that is something why what we are talking 
about today is so important.
    Mr. Lance. Thank you. If there are fewer choices among 
these platforms, how does that change the evaluation of the 
platform's ability to moderate content? Does it make it more or 
less troublesome in your judgment?
    Ms. Klonick. Yes. I think that as Representative Doyle said 
earlier, that one of the issues here is that there is a lack of 
choice between certain types of providers, but on these 
platforms right now there is just a plethora of choice. I mean, 
Twitter might have a monopoly over 280 characters of text and 
Facebook might have a monopoly over a kind of like a relatively 
safe, family-safe community, but there are plenty of other 
presences that are currently online. Of course, if that changes 
in the future and the taxonomy of what these different 
platforms are able to provide and what users use them for and 
how they end up having a monopolization or not over broader 
areas, then I think that that is something that can be 
revisited.
    Mr. Lance. Thank you very much.
    Professor Ben-Shahar, as many of the online platforms we 
are discussing today offer their services free of charge to 
consumers, how do we as lawmakers evaluate the appropriate 
balance between personal privacy against convenience?
    Dr. Ben-Shahar. Thank you very much for the question. I was 
hoping to be able to say a few words about that. I think we 
should be very careful not to change this grand bargain, people 
paying for excellent services that they like very much not with 
money but with their data. And it would be a, I think, disaster 
of consumer protection if we changed that, if you ask consumers 
in the aftermath of some reform that removed that bargain and 
made them pay for things like Google, Facebook, and other 
things with money, if they feel that they were helped, I think 
they would say in unison, ``No, don't do this.''
    In that sense, I think the bargain and the underlying 
bargain is an excellent bargain. Now, there are worries that of 
course arise, and I think this is the ultimate, the 
foundational problem of data policy. It is not privacy or 
security, it is competition. It is the fact that there are very 
few companies that dominate the central forum in which these 
exchanges occur--Google, Amazon, Facebook, and maybe a few more 
small players.
    I am not so worried about the ISPs. They, notwithstanding 
the fact that on broadband there is some local monopolies, 
there is great competition from mobile, but these big three, or 
big four if you throw in Apple, big five if you throw in 
Microsoft, have a lot of power, and the FTC has failed, for 
example, last year, to intervene in something that the 
Europeans thought, I think rightly, as raising antitrust 
concerns.
    So to conclude, I think that the concern for consumers will 
arise from lack of competition and concentration, not from 
privacy and security.
    Mr. Lance. Thank you very much, and I yield back 42 seconds 
and I recognize Mr. McNerney of California.
    Mr. McNerney. Well, I thank the chairman and I thank the 
witnesses. Sorry, I missed some of your testimony a little 
earlier. Professor Moy, what do you think the benefits of the 
current FCC rules for consumers and small businesses are 
regarding net neutrality?
    Ms. Moy. Great, yes. So I mean that is a great question. I 
appreciate that question. The current rules enable small 
businesses to reach consumers. That is the short answer to the 
question. You know, if we didn't have rules that prevented ISPs 
from paid prioritization and blocking, then it would be much 
more difficult, potentially, for small businesses to reach 
consumers.
    Mr. McNerney. So you would agree--or I don't want to put an 
answer in your mouth--would you agree that it would be harder 
for small businesses to innovate if the FCC Chairman's proposal 
is adopted?
    Ms. Moy. Yes. You know, and it might even be very difficult 
for a business to know whether or not it is being throttled if 
it is being throttled. The draft order has transparency 
provisions in it, but it is unclear whether the transparency 
provisions would be consumer-facing or in fact if some 
companies could fulfill those by just turning over information 
about their practices directly to the FCC.
    Mr. McNerney. Well, that sort of leads, already answered my 
next question. But the new rules or the new regime would 
require or ask businesses if they feel like they have been 
subject to anti-competitive practice to go to the FCC to 
resolve their problems. How quickly do you think the FCC could 
response to those sorts of requests?
    Ms. Moy. I mean, if it could respond at all, I mean, well, 
I think the question is whether it could respond at all, right. 
So there are many practices that might seem anticompetitive but 
not raise to the level of an antitrust violation. So, for 
example, if an ISP were throttling a service that an innovator 
is introducing into the market but that doesn't compete 
directly with the ISP service of a phone or internet provision, 
then that practice might look anticompetitive but might not be 
considered an antitrust violation.
    Also if, you know, if a company were to try to bring an 
action in court, you know, I think there is this idea that 
companies might be able to bring antitrust actions in court, 
but antitrust actions in court take many years and may cost 
potentially millions of dollars to mount against a major 
incumbent. And that can be, you know, that is a barrier that 
really creates impossibility for a small business or----
    Mr. McNerney. Sure. And what sort of penalties could the 
FTC impose, and would they be effective?
    Ms. Moy. Right. I mean, so again the FTC's primary 
authority when it comes to enforcing something like net 
neutrality, if it could enforce net neutrality again, you know, 
and I think for all of the reasons that we have discussed 
repeatedly, including the FTC's lack of authority over common 
carriers, it is questionable whether they have the authority at 
all, but most of their authority would come from the ability to 
prohibit unfair and deceptive trade practices, and there is no 
civil penalty authority in that area.
    Mr. McNerney. So under Chairman Pai's plan, broadband 
providers are not required to disclose the practices at the 
point of sale or on their website, but they can give those 
practices to the FTC and the FCC, and they would in turn put 
them on their website. Is that sort of disclosure viable?
    Ms. Moy. So, you know, I mean, I think I would say again, 
you know, I think as an initial matter it is worth remembering 
that the disclosures alone are not necessarily, are not going 
to be sufficient, particularly when it comes to when you are in 
a situation where a consumer only has access to one broadband 
provider.
    But when there is a choice that is available to the 
consumer and they might rely on disclosures to make a choice 
between two different providers or between multiple providers, 
that information really does need to be consumer-facing. I was, 
in fact, on the task force at the Consumer Advisory Committee, 
the FCC's Consumer Advisory Committee that designed the so-
called broadband nutrition label that Chairman Pai is planning 
to do away with, you know, and we did think that in a situation 
where a consumer might be considering adopting one of two 
different services or one of two different service plans, it 
would be extremely important for them to have easy-to-read 
information about the actual performance of that service 
package.
    Mr. McNerney. I had a couple of questions for Professor 
Kearns. With regard to machine learning, there are going to be 
benefits in all sorts of areas, but are there areas where 
machine-learning techniques should not be used?
    Dr. Kearns. Well, yes, I think so. And there is, you know, 
a large and growing community of AI and machine-learning 
researchers who are trying to debate those sorts of issues. You 
know, one logical extreme, there is the notion that any 
decision that really, you know, should lie with a human just 
because of moral agency shouldn't be made by an algorithm.
    So one example that is commonly offered is in automated 
warfare, that even if we could design algorithms or learn 
models that, you know, made more accurate decisions about 
whether to fire on an enemy, that perhaps we shouldn't do that 
because the decision to do that should always lie with a human 
who has the moral responsibility for that decision.
    So I think, you know, that is an extreme that I think I 
would agree with. The harder cases, I think, are cases in 
which, you know, machine learning is demonstrably effective yet 
making difficult moral decisions like in criminal sentencing 
and to, you know, one could arguably ask about things like, you 
know, college admissions or loan decisions and the like.
    And so, you know, my view right now is that we are at the 
very beginning of a very difficult debate about the extent to 
which decisions that have been made historically by humans and, 
by the way, you know, historically also exhibited biased 
privacy decisions, et cetera, when they were being made by 
humans and turning over them to machines where the tradeoffs 
are going to be different, but there will be tradeoffs, right.
    And there is always this tension in machine learning 
between accuracy, which is, you know, right now essentially 
what is almost always optimized for, and other things like 
privacy or fairness, right.
    Mr. McNerney. Well, I have really gone over my time.
    Dr. Kearns. OK, yes.
    Mr. McNerney. So I am going to have to interrupt you and 
yield back. Thank you.
    Mr. Lance. Thank you very much. The Chair recognizes Mr. 
Johnson.
    Mr. Johnson. Thank you, Mr. Chairman.
    Mr. Pasquale, when we talk about how companies interact 
with consumers and handle consumer data, we often talk about 
transparency, that is, being transparent with business 
practices. In some industries there are actually transparency 
rules that require companies to disclose certain information. 
For example, ISPs have to disclose a slew of information about 
their business and network practices. Are there any rules that 
require companies that use algorithms to be transparent about 
how they work?
    Mr. Pasquale. So it is a very narrow range of requirements. 
So, for example, if you look at the online lending space, there 
has been some caution about certain forms of automated 
underwriting using what is called fringe or alternative data, 
data beyond, you know, what is usually used by FICO or other 
entities like that because under FCRA it can be a requirement 
of explanation under the Fair Credit Reporting Act with respect 
to some of these, like giving the reason codes for why an 
automated decision was made.
    But in general it is a zone of great opacity. We just don't 
know. That is why I titled my book ``The Black Box Society,'' 
because there are so many rules there, so little sense of what 
is going on there. Yes.
    Mr. Johnson. OK, all right.
    Mr. Kearns, Professor Swire from Georgia Tech--my alma 
mater, by the way, it is where I learned about networking--
concluded that applications such as search engines and social 
networking services collect data providing greater consumer 
insight than ISPs. Do you agree with that conclusion?
    Dr. Kearns. Yes, I do.
    Mr. Johnson. OK, care to expand?
    Dr. Kearns. Well, in addition to the aforementioned 
encryption that, you know, occurs with the vast majority of 
data that ISPs carry, you know, there is the additional fact 
that I don't think it has been mentioned yet that it is at the 
packet level. And the way internet routing, packet routing 
works is that longer messages, whether they are actual text 
messages or they are a web search or they are an audio call, 
are divided into these tiny little fixed-size packets which 
then travel possibly different paths through the network.
    So, you know, just going back to a comment I made earlier, 
this sort of actionability of data at that level, if half or 
more of it is encrypted and it is also traveling in these 
little bite-sized pieces and you are carrying a phenomenal 
amount of that data over your network, if you ask me whether if 
I am trying to figure out who somebody is and what to sell them 
and what their mental and psychological condition is, I would 
much rather have search engine data or Facebook data than 
packets at the network level.
    So this is basically what I mean by, I think, you know, 
from a privacy perspective it is less concerning to me than the 
data that is being collected by the edge services.
    Mr. Johnson. OK, all right. Continuing with you, Mr. 
Kearns, then, my understanding is that approximately 80 percent 
of internet traffic is encrypted. You just talked about 
encryption a little bit. That limits what ISPs see regarding 
consumers' online activities. In contrast, by their very 
nature, don't edge providers largely have to interact with 
consumers' unencrypted data?
    Dr. Kearns. By definition, yes.
    Mr. Johnson. Yes. Well, doesn't that give edge providers 
much greater insight into consumers' preferences, habits, 
choices, beliefs, that kind of stuff?
    Dr. Kearns. Yes, it does. I mean, I think the right way to 
think about it, let's say, back in the old days of telephony 
is, you know, would you rather see the raw analog signal and 
try to figure out what the conversation is from that, or would 
you rather have that analog signal rendered through a speaker 
so that you could actually listen to the conversation, right? 
And this is an imperfect metaphor, but I think it is a good 
one.
    You know, another thing I might offer is, if I am just 
trying to describe an image to you, would you rather I go pixel 
by pixel through the image and tell you the color value of it, 
or would you rather me describe it to you and say, well, ``It 
is an outdoor image''? ``There are trees. There is a lake. 
There is a family picnicking.'' And so, you know, by 
definition, what the end services are getting and what users 
want to give to those end users are this much-higher-level data 
that is easy for humans to understand and model.
     Mr. Johnson. They want to see it all put back together 
again.
    Dr. Kearns. Yes, exactly. And you are just kind of not 
easily getting that at the network level because of the 
encryption and because of the fragmentary nature of packet 
routing.
    Mr. Johnson. Right, right. OK.
    Well, Mr. Chairman, I yield back a full 10 seconds.
    Mr. Lance. Thank you, Mr. Johnson. The Chair recognizes Ms. 
Eshoo.
    Ms. Eshoo. Thank you, Mr. Chairman, and thank you to the 
witnesses. I read all your testimony last night, listened to 
all of you today, and I want to make some comments about this 
hearing. The title of it is very interesting, and it is an area 
that needs to be examined.
    The word ``privacy'' has come up many times, certainly net 
neutrality and references to it have come up. Strong 
enforcement has come up. But when you look at the backdrop and 
the broader stage on which this hearing sits, look what is 
happening in our country. In a flash, like lightning, privacy 
was ripped away, the privacy protections were ripped away from 
the internet.
    So all of the happy talk of some of my colleagues on this 
committee about privacy and the sanctity of it, that was 
forgotten when that vote was taken and the American consumer, I 
think, has really been hammered as a result of it. I think 
that, Professor Moy, you made a very important point when you 
said that the last thing we should do is to repeal, and that 
has happened.
    It was very interesting to hear the description of what has 
taken place in Europe with what they have done with the 
internet and what we have done and how the internet has 
flourished in our country just on the eve of the Chairman of 
the Federal Communications Commission getting ready to rip away 
the protections that are there that have made it open, free, 
accessible. So I think there is some political cross-dressing 
here today, with all due respect, not by the panelists, but I 
think by some of the Members.
    And the term ``a strong enforcement'' has been referred to, 
but I don't think strong enforcement is something that you pick 
and choose, because we are lawmakers and, unless there is 
enforcement, then the law is not worth the paper that it is 
written on.
    I take heart from what Professor Kearns spoke of because, 
in this whole issue of algorithms--and let's keep in mind these 
social platforms are free. They are free. They are not like the 
ISPs. In the ISPs there must be, I think, only three happy 
outfits in the entire Nation on the eve of what Chairman Pai is 
doing relative to net neutrality, and that is Comcast, AT&T, 
and Verizon. They are the happiest. I don't know anyone else 
that is for what he is planning to do to the internet. But I do 
think that it is very interesting to me that you have raised 
the issue of auditing algorithms.
    Now, I think that truth has always required transparency. 
We don't, I don't think, as a committee, really know how to get 
socks on the octopus, so to speak, here because it is 
complicated. Free speech is central to us, but we also know 
that there are bad actors that have used the best of what we 
have invented to divide us, and something needs to be done 
about that. There is no question in my mind, and the chairman 
of the full committee raised that, as well.
    So how close, Professor Kearns, do you think we are to this 
what you raised, the auditing of algorithms?
    Dr. Kearns. So I think we are close. So in particular, you 
know, many of the instances of discrimination, for instance, in 
algorithmic behavior were actually discovered by groups of 
researchers who are effectively doing their own auditing, you 
know, doing kind of field experiments using services that have 
algorithms underlying them, testing their behavior and 
demonstrating, for instance, they have some particular type of 
bias.
    There is good research being done on, again, internalizing 
notions of fairness inside of algorithms. And just to be clear 
here, I think most instances of discrimination in algorithmic 
behavior are not the result of any evil by the researchers and 
scientists at these companies. It is just that, when you 
optimize your model for predictive accuracy, you shouldn't 
expect it to have any other nice properties, either, so you 
need to actually specifically put those properties in your code 
if you want them to have it.
    You know, in the privacy arena there is a very strong 
notion of, you know, kind of internal privacy of an algorithm 
known as differential privacy that is kind of starting to 
finally get out of the lab and be used, for instance, in the 
latest version of Apple's iOS. So this stuff is happening, and 
the tech companies are participating in, you know, the dialogue 
and in developing some of the science. It just needs to be kind 
of taken seriously at scale by those companies.
    Ms. Eshoo. Well, I am encouraged by what you have just 
described, and I want to pursue it, as well. If there is more 
information that you can get to us on it, I certainly would 
welcome it. And with that, Mr. Chairman, I yield back.
    Mr. Lance. Thank you very much. The Chair recognizes Dr. 
Bucshon.
    Mr. Bucshon. Thank you, Mr. Chairman.
    Professor Kearns, this is a little bit different line of 
questioning but important. Is it feasible for your cell phone 
or an app on your cell phone to listen in on your conversation 
and collect data?
    Dr. Kearns. Yes.
    Mr. Bucshon. And are you aware that that is happening in 
our country, in everywhere? I will give you an example of why I 
think that this is happening, and it is an issue that we really 
haven't touched on today as part of data collection.
    Dr. Kearns. My default assumption is that, unless I have 
taken explicit pains to arrange otherwise, that when I use an 
app on my mobile phone, it is recording at least the data of my 
interaction with that app and possibly many other aspects of my 
usage of the phone, as well.
    Mr. Bucshon. How about when you are talking? Like right now 
my phone is sitting here, and there is a speaker and I am 
talking, and is that data, is what I am saying potentially 
being collected?
    Dr. Kearns. With or without the microphone on.
    Mr. Bucshon. Correct, with or without. Well, the question 
is the definition of ``on,'' right, because that is being made 
by the company that makes the phone. I mean, it has been shown 
recently and it has been on, I think, Wall Street or somebody 
reported that you can turn off essentially everything on your 
phone and you are still being tracked. So the speaker is 
important.
    Let me just say this, and this is the reason this came to 
me is because my son, who is 24, he lives in Chicago, he was 
standing around with some, a couple, with a friend at work, a 
person at work. Nobody was on the internet. He was talking 
about, and I can't remember specifically what it was, but it 
was about shoes or something and the next day he had ads for 
that exact thing on his feed. He didn't do a Google, he didn't 
do any search. I don't want to single out a company, but he 
didn't do any search at all. All he did was talk in the 
presence of his microphone on his phone. Do we know if that is 
happening?
    Dr. Kearns. I am not a security expert, but I do know that 
there are more instances these days of situations in which, you 
know, the operating system on your mobile phone communicates 
with beacons in retail stores, and this is how one often 
experiences, you know, why even though I didn't do a search on 
some product at all, but I happened to be in the store 
yesterday, the physical retail store----
    Mr. Bucshon. Yes, they can do that.
    Dr. Kearns [continuing]. Am I not, you know, and this is 
because they are now starting to install so-called beacons in 
these stores that interact with the operating system on your 
phone, and so then the retailer knows that you were there.
    Mr. Bucshon. If you were in a shoe store, they know you 
were in a shoe store.
    Dr. Kearns. So, you know, my feeling about these things is 
that the way technology is, is anything is possible, right. And 
then the question is, is it widespread and who is doing it, and 
is it kind of for deliberately nefarious purposes or is it, you 
know, just advertising, quote unquote?
    Mr. Bucshon. I mean, it is important because I am a Member 
of Congress and I have confidential conversations all the time 
with my phone, and I am not on the internet. And so that is a 
question. I had mentioned this to my staff, by the way, when I 
went back to the office, and they go, ``Oh yes, that has 
happened to me.'' I mean, all the young people are like, ``Oh 
yes, that happened to me before.''
    So I just thought that was something that we need to, 
really, also as far as collecting data and then analyzing like 
you have described, I mean, I think what we really need to 
think about, not only when you are actively on your phone but 
whether or not through your--and I am not a conspiracy theorist 
or anything, right--through your actual speaker that you can be 
monitored.
    Dr. Kearns. Yes. I mean, and I think we are also 
voluntarily heading this direction in the form of home devices 
like, you know, Echos and, you know----
    Mr. Bucshon. Yes, right. That is obvious, right.
    Dr. Kearns [continuing]. In which, you know, are kind of 
sitting there all the time recording.
    Mr. Bucshon. Right.
    Professor Ben-Shahar, you stated that consumers ignore 
privacy disclosures. How would you suggest we inform consumers 
that they have given consent to their data being collected? How 
can we do that?
    Dr. Ben-Shahar. I think consumers understand in general 
what is going on, and indeed a lot of the surveys suggest that 
they know that a lot of their information is being collected. 
They are not surprised when they find out that yet another 
practice is prevalent, for example, that now these home 
butlers, the Google Home or Alexa is listening to everything 
that is going on. I think that consumers by now have figured 
out that this is going on, and so there is not much that we can 
tell them that they don't know.
    Now there are specific things that are going on that defy 
consumers' expectation. And if the expectation is created in an 
affirmative way by your smart phone or by Google or by other 
service, for example, they give the consumer the impression 
that they can turn on or turn off some kinds of surveillance or 
some kinds of data collection and it turns out that they can't, 
that even if they did what they were supposed to do and had the 
reasonable understanding that they are not going to tracked in 
a particular way, they still are, that is an FTC issue. That is 
an issue of----
    Mr. Bucshon. Well, that has happened. It has just been 
written in the papers recently that it has happened.
    Dr. Ben-Shahar. To the extent that that is happening, that 
should be--I think that there are tools in our law, both in 
contract law and in consumer protection statutory law, to take 
care of these kind of things. I don't know, you know, maybe 
other panelists know better. I don't think these things happen 
too much, for the simple reason that it all costs nothing for 
the services to let consumers know what is going on. Consumers 
don't care. They are not going to bother, change the settings 
or re-change the settings every time there is a new version of 
the software.
    Mr. Bucshon. Thank you. I am out of time. I yield back.
    Mr. Lance. Thank you very much.
    The Chair recognizes Congressman Flores.
    Mr. Flores. Thank you, Mr. Chairman. I appreciate all the 
panelists for joining us for this important hearing today.
    The first question I ask, I mean, one of the things that is 
obvious is that data is pulled from everywhere, whether it is 
data services, your mobile phone, your Alexa, whatever, 
operating systems, and social media platforms. So my question 
is this, for all of the panel. I am going to start with 
Professor Kearns, and then I am going to ask a couple of other 
questions, and we will come back to the panel if we have time 
about this issue.
    So the question is simply this: What are your thoughts as 
to whether or not Congress or policymakers need to establish a 
consistent legal and regulatory framework for how this data is 
obtained and used?
    Dr. Kearns. Well, I will be brief so other people can talk, 
too. But, I mean, as per my earliest remarks, as a scientist, 
so I am not a policymaker, I am not a lawyer----
    Mr. Flores. Right.
    Dr. Kearns [continuing]. But from a scientific perspective, 
to me the most important thing is not sort of, you know, how 
much data you have measured in petabytes. It is not kind of 
whether the data came from this service or that service or this 
app or that ISP. It is, what are the actionable insights about 
consumers and what are the facts about their lives that you can 
infer from that data?
    And as a scientist I don't see an easy way to carve that up 
into little subdomains and say, like, ``Oh, well, you know, 
because we just--'' the truth is, we don't know, right. These 
companies themselves are figuring out just now how powerful AI 
and machine-learning techniques applied to all kinds of data 
are.
    Mr. Flores. Right. Well, the challenge is, is that 
policymakers and regulators typically move way behind the speed 
of technological change. And so what I am trying to figure out 
is how do we get in front of this, or do we need to even worry 
about it? And I will come back to the rest of the panel on this 
question in just a minute, but I do have two other questions 
for Professor Pasquale first.
    In your testimony, you noted that bottlenecks can threaten 
competition at any layer of the network, not just the physical 
layer provided by the ISPs. And so the question is this: Can 
you elaborate on the potential bottlenecks other than the ISPs, 
beyond the ISPs?
    Mr. Pasquale. Sure. So I did a 2008 article called 
``Internet Nondiscrimination Principles,'' and what I tried to 
do is to say that the same type of concerns that are motivating 
people to advocate for net neutrality should also be looked at, 
at the social layer, at the search engine, at the app store 
level. And particular examples, there are two examples related 
to China that I think are really interesting and I discuss in 
my book. One is that someone developed an app called ``In a 
Permanent Save State,'' and it was a game that was also a 
critique of Apple and its use of certain Chinese factories and 
labor. And the Apple app store rejected it over and over again, 
and they couldn't really understand why that was happening.
    Similarly, there is a case called Langdon v. Google where 
someone wanted to buy an ad titled ``China is Evil,'' and there 
was, I thought, a relatively arbitrary decision by Google to 
say, ``No, we are not going to sell you that ad.'' And so I 
think those are very concrete examples of a much larger 
problem, where I think that we have to be much more imaginative 
as academics and as policymakers in seeing the connections 
rather than seeing the separations between these different 
entities.
    Mr. Flores. Well, that sort of goes to my next question, 
because we have talked a lot about how content is filtered 
online, but we need to consider how content is filtered through 
other platforms, even voice service devices. It has been 
reported that voice service devices prioritize certain content 
and services and they have even excluded certain products from 
their platforms.
    So the first question is, are there anti-competitive 
concerns associated with this type of prioritization?
    Mr. Pasquale. Congressman Flores, I have to confess I am 
not familiar with that niche of the market, so I will have to 
pass.
    Mr. Flores. OK. That is fine. Let's move back to my initial 
question, if we can. I would like to get the comments from the 
rest of the panel. Again, the question was this: What are your 
thoughts as to whether policymakers need to establish a 
consistent legal and regulatory framework for how this data may 
be obtained and used? Let's start with Ms. Tucker.
    Dr. Tucker. So I think it is very difficult--and Europe has 
taught us this--to have a consistent framework governing 
technology. On the other hand, I think it is possible to 
identify areas where we are particularly concerned about 
privacy, be it health, be it kids, and make sure the policy is 
focused on protecting those outcomes we really care about.
    Mr. Flores. OK.
    Dr. Ben-Shahar?
    Dr. Ben-Shahar. My answer, with all due respect, is a 
resounding no. I don't think that policymakers should tell 
business what data to collect and how to use it.
    Mr. Flores. In the interest of time, I appreciate the short 
answer, OK.
    Dr. Ben-Shahar. And maybe just set red lines.
    Mr. Flores. Ms. Klonick, sorry.
    Ms. Klonick. Yes. I think that regulation, Section 230 and 
any regulation that kind of curtails the ability of these 
businesses and platforms to self-regulate, is probably not in 
the best interest of the public.
    Mr. Flores. OK, thank you.
    And, in the interest of time, I will yield back the balance 
of my time.
    Mr. Lance. Thank you very much. The Chair recognizes 
Congresswoman Walters of California.
    Mrs. Walters. Thank you. And thank you for holding this 
hearing, and thanks to the witnesses for being here.
    We can all agree that protecting consumers' information is 
paramount and that consumers deserve a clear understanding of 
their privacy expectations when using the internet. It is 
important we have this discussion so we can better understand 
how consumers benefit from current practice and examine ways to 
protect against the misuse of consumer information.
    Professor Tucker, what is the best way to protect my 
constituents' privacy to make them feel secure and confident in 
the use of their data without impeding future innovation and 
America's leadership in the technology sector?
    Dr. Tucker. So, over the various sectors and various time 
periods, my research has repeatedly shown that the best way of 
introducing privacy protections is to give a sense of control 
back to consumers. Now, that is distinct from transparency. It 
is distinct from disclosures. Instead, it is about restoring a 
sense of control. And what is more, my research has actually 
shown that that kind of policy is in from self-interest. And if 
you try and do the kind of microsegmentation using really 
personalized data, for example, preferences of someone over 
shoes, then using that kind of data for advertising only works 
if there is a parallel sense of control among consumers.
    Mrs. Walters. OK, thank you.
    Professor Ben-Shahar, what protections do existing legal 
schemes provide for consumers to protect them from the theft or 
loss of their data, and are those legal schemes sufficient?
    Dr. Ben-Shahar. Well, I think that, again, I am not a data 
security expert, but my understanding is that there are very 
few protections that are granted to consumers. Many of the 
things that were recommended that people do after, for example, 
the Equifax breach were fairly limited. I mentioned before in 
my testimony that I think that the reason there are so few 
remedies and recourses is because largely there is no evidence 
for the fact that consumers are suffering in a magnitude of 
harm that requires greater a remedy in this context.
    Mrs. Walters. OK. And then I have another question for you. 
How does the use of algorithms to deliver content impact 
consumers' experiences online, and is there a benefit we see to 
the practice of collecting data?
    Dr. Ben-Shahar. I think that benefit is enormous, and it 
has been, you know, measured in many different ways. But I will 
just recommend to try one time to disconnect all the knowledge 
about you from your smart phone and see what happens. When you 
open Google Maps and want to go something and it no longer 
recognizes after the first letter where it is that you wanted 
to go and the inconvenience that you will say, ``Ah, no, I wish 
the data service was still on, the recognition was on.''
    I think in many contexts personalization delivers 
astronomical value that has not yet been tapped. In my own 
research I am looking about at ways in which we can personalize 
legal rules and other things, but the only reason that we think 
about these new areas is because existing areas have proven to 
be enormously beneficial--education, insurance, medicine, and 
the like.
    Mrs. Walters. OK, thank you.
    And Professor Tucker, some digital platforms would say 
that, when third parties are permitted to use their platform, 
that platform gives consumers the tools to control their 
experience. Are we putting too much of the onus on the consumer 
to review the permissions the developer is requesting and 
forcing the consumer to choose which information to share?
    Dr. Tucker. So I think this is a very good distinction to 
make in that, let's be clear, whenever we have actually studied 
search logs of how consumers behave when they are confronted by 
control, rather than opting out and, you know, protecting their 
privacy, they tend to actually go in and try and improve the 
data, because there is nothing more irritating--I don't know if 
this has happened to you, that you are looking at a web service 
which thinks you are a 25-year-old man, and you are like, ``Why 
do you think that?'' Consumers tend to try and improve the 
quality of data, intriguingly.
    The one distinction I do want to make, though, is that 
there are some categories of consumers where perhaps there 
isn't that level of control exerted. For example, we have a 
study right now which looks at apps which are targeted at 
toddlers. I don't know if you have ever been to a restaurant 
where parents are using these to quiet down their toddlers, but 
we saw there a vast quantity of data being collected. And there 
I think it is fair to assume that those toddlers are not really 
actually exerting any control on whether their location is 
being tracked or their use of the sort of My Little Pony app or 
whatever it is.
    Mrs. Walters. Thank you, and I yield back the balance of my 
time.
    Mr. Lance. Thank you very much. The Chair recognizes 
Congressman Costello.
    Mr. Costello. Thank you. I want to share some reflections I 
have here and allow each of you to correct my understanding or 
enhance it, whatever terminology you may wish to use. From my 
perspective, browser history in some respects is a commodity, 
but it is very invisible and at this point there is no 
regulatory framework for when and how it can be incorporated 
into an algorithm.
    I take, and this is not a precise corollary, but if I made 
a phone call to you and the content of our discussion was 
transcribed and it was then sold or utilized for proprietary or 
commercial gain, there are some similarities between that and 
how an ISP is able to gather some of that content and then 
incorporate that into an algorithm or into how advertising 
would make its way into my internet searches, or if I go to a 
news website, all of a sudden up pops laundry detergent if I 
was Googling laundry detergent.
    Someone made the comment about editorializing content or 
raise concerns on the political side. It may have been Ms. Moy 
in her written testimony. I read everyone's written testimony. 
The trouble, the thing that I am grappling with on the concerns 
related to what kind of political content shows up and how you 
might be able to shape one's opinion of things is, what is the 
difference between that and picking up a newspaper in the 
morning? And I don't really know how to distinguish between--
you can distinguish between the two, but in some respects I 
don't know that you should distinguish between the two.
    As it relates to the Federal Trade Commission, if we are 
talking about, particularly on political content, but even 
amongst other things, how would the FTC go about adjudicating 
equal time if we were to get into talking about political 
content, and how does it get, how do you determine, oh, well, 
you put too much left-leaning or too much right-leaning 
content? I think that that can get deeply problematic.
    And I believe, also, Ms. Moy mentioned something about 
adding protections for consumers, if you could share with me 
what kind of protections you might be speaking about.
    The gentleman, I believe it was Dr. Ben-Shahar, I agree 
with your testimony. I don't think that these waivers or 
disclaimers or--it doesn't mean a hill of beans. I totally 
agree with you. I am not sure, I think that is just more about 
indemnification or protecting one's liability, and that is 
fine. I mean, I don't think we should expect more from that. I 
don't know how you could expect more from it.
    But the final thing I want to say for comment relates to 
Ms. Tucker's testimony. And in the final two paragraphs, you 
talk about how different types of data can have different 
consequences and that any regulation, rather than treating all 
the data the same, needs to distinguish between what kinds of 
data may be actively harmful to consumers and what data may not 
be.
    And it seems to me that we are really talking about values 
here, right. We want algorithms to be able to be helpful to the 
consumer and, candidly, in some respects helpful to those who 
are going to use that data to make sure that you have 
information that you may be more predisposed to wanting to see. 
We don't want that data to be harmful.
    See, I am going on way too long. How do we create a clear 
yet evaluative standard and entrust everyone to follow it with 
enough tools for the FTC to embrace that kind of framework if 
we were to do it? I have spoken way too long. Comments?
    Mr. Pasquale. I mean, I just want to--I have two quick 
responses, one being that I do think that, you know, in terms 
of thinking about what data is sensitive and what is not, that 
can be a strength of a privacy regime.
    But if we also look at the work on big data proxies, how 
like Nicholas Terry has described, how you can have, say, 
location could be a proxy for race or the very data that you 
don't think is terribly sensitive could be a proxy for other 
data that is sensitive, that is where I would turn sort of Dr. 
Kearns' work against Dr. Tucker's work in a way and sort of say 
that there is a way in which, you know, it is because of these 
sort of inferences you can make from somewhat insensitive data 
to sensitive that is important.
    With respect to Google and the newspaper, the difference 
that I would make is that I would say that what we are 
concerned often with respect to unfair algorithmic influence on 
political activity would be something that was a lot more 
subtle. So, for example, imagine if Facebook decided it was 
only going to encourage Democrats to vote. We do have studies 
that have shown that that can lead to I think it is a 0.63 or a 
small increase on the margin of the people whose feed is spiced 
with get-out-the-vote advertisements.
    So that is something I think we definitely have to look for 
because, when a newspaper says ``Vote for X,'' I can see that. 
But when Facebook, you know, suddenly spices the feed of the 
people that, say, it likes, then we can't see that.
    Mr. Costello. Fair point.
    Ms. Moy. So yes, and I will just add, you know, when it 
comes to--so a couple things. One, you know, when it comes to 
the FTC's enforcement authority, at the risk of sounding like a 
broken record, the enforcement authority really is limited to 
deception, unfair and deceptive practices, and there is no 
civil penalty authority.
    But, you know, on your question of paid political ads, 
specifically, you know, I think that this is a really hard 
challenge that I suspect we don't have a lot of really good 
answers for yet on how to deal with. You know, one thing, 
though, is that there is very little transparency about what 
ads are being paid for and even when they contain political 
content. The FEC is conducting a rulemaking right now to at 
least explore the possibility of increasing transparency when 
it comes to labeling of political content on platforms, but--or 
online, I should say.
    You know, but I think also this is a question where it 
might be extremely difficult to identify some political 
content, for example, when it relates to issues as opposed to 
candidates, without human eyeballs. And there is a tremendous 
amount of content that gets posted online and not nearly enough 
human eyeballs reviewing some of that content to determine 
whether and to what extent it might have a political effect.
    Mr. Costello. I am just going to read this, something real 
quick into the record. I know you are ready to get out of here, 
Mr. Chairman. When someone states, quote, ``I could slow 
down''--well, we talked a lot about power that exists in the 
hands of those that are not ISPs. For instance, just last 
weekend, Matthew Prince, the CEO of Cloudflare, signaled he 
would look into taking up a challenge to slow down the FCC 
Chairman's internet speed at his home. These apparently are not 
the least of the threats to Chairman Pai's home life.
    When someone states, quote, ``I could do this in a 
different but equally effective way''--and I would like to 
submit the entire string of tweets for the record--isn't it 
clear there is a great deal of power in those that are not 
governed by the same rules in the internet ecosystem? And how 
would your reaction be different if an ISP did this rather than 
an edge provider?
    We don't have time, but if we could take any comments for 
the record on that, because we are dealing with this larger net 
neutrality issue, and I think some of the concerns are that it 
is not just ISPs that we should be looking at. There are some 
others that aren't governed that clearly have the power to do 
things that we all have concerns about. I yield back.
    [The information appears at the conclusion of the hearing.]
    Mr. Lance. Thank you very much, Congressman Costello.
    Seeing there are no further Members wishing to ask 
questions, I thank all of our witnesses for being here today. 
Before we conclude, I include the following documents to be 
submitted for the record by unanimous consent: a paper from the 
21st Century Privacy Coalition, a letter from the Electronic 
Privacy Information Center.\1\
---------------------------------------------------------------------------
    \1\ The paper has been retained in committee files and also is 
available at  http://docs.house.gov/meetings/IF/IF17/20171129/106659/
HHRG-115-IF17-20171129-SD004-U4.pdf. The letter appears at the 
conclusion of the hearing.
---------------------------------------------------------------------------
    Pursuant to committee rules, I remind Members that they 
have 10 business days to submit additional questions for the 
record and I ask that witnesses submit their response within 10 
business days upon receipt of the questions. Without objection, 
the subcommittee is adjourned.
    [Whereupon, at 12:47 p.m., the subcommittee was adjourned.]
    [Material submitted for inclusion in the record follows:]

[GRAPHIC(S) NOT AVAILABLE IN TIFF FORMAT]

    
                                  [all]
                                 
                                 
                                 
</pre></body></html>
